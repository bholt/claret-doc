# Mitigating Contention with Abstract Data Types
<!--
All together, the techniques described so far provide many useful tools for exposing concurrency and replication to mitigate contention.

However, these techniques require information about the application or the programmer's intentions. Abstract locks require knowing the commutativity properties of operations. Consistency SLAs require the programmer to choose a set of SLAs for each operation. Quelea and Indigo both require programmers to write different annotations.
-->

In order to perform the many contention mitigation techniques available, the system must have knowledge of the desired application semantics.
Our proposed solution to this problem is an old one: *abstract data types* (ADTs). The core idea of ADTs is to present a clear *abstract model of state and behavior*, while hiding all the implementation details.
With ADTs, applications describe their semantics to the underlying system *by construction*, allowing it to take advantage of properties, such as commutativity, to reduce coordination, avoid conflicts, and improve performance.
All the details of ordering constraints, visibility, and coordination can be hidden behind the abstraction of data types with well-defined behavior. 

ADTs are a natural interface for developers to express application semantics.
They understand how a `Set` ADT behaves, and the system knows from a specification like [#tab-spec] that `add` operations always commute with one another but only with `size` under some circumstances. Programmers can maximize the optimizations available to the system by selecting the most specific ADT for their usage. For instance, an application wishing to generate unique identifiers should prefer a `UniqueIDGenerator` over a generic `Counter`: incrementing a `Counter` must return the next number, but a `UniqueIDGenerator` lifts that restriction so can generate non-sequential IDs in parallel. Programmers can even provide their own application-specific ADTs or customize existing ones to make them more suitable.

The concept of ADTs has long been used to extend databases: supporting indices and query planning for custom data types [@Stonebraker:83;@Stonebraker:86], concurrency control via abstract locks [@Herlihy:88;@Chrysanthis:91;@Badrinath:92;@Weihl:88:ADT].

Today's distributed systems deal with new challenges, and have evolved the many techniques described above to solve them. Unfortunately, many of the lessons learned about the benefits of the ADT abstraction were not carried forward into modern distributed systems. Many were lost in the move from relational databases to "NoSQL" datastores. We are just now beginning to figure out how to leverage type-level semantics in systems with weak replication through CRDTs and Bloom.

My work has pushed for the use of ADTs to allow systems to better mitigate the capricious, high-contention situations that cause so much trouble to applications. In Grappa, a high-performance system for irregular data analytics, we used *combining* to improve throughput on globally shared data structures. In *Claret*, we showed how ADTs can be used to improve performance of distributed transactions. Finally, I will propose *Disciplined Inconsistency*, a way to safely trade off consistency for performance with approximate ADTs.

## Combining with global data structures in Grappa
Grappa [@grappa-usenix15] is a system we built for irregular data analytics. 
In order to tolerate the latency of communicating between nodes in commodity clusters, Grappa requires significant concurrency; luckily applications like graph analytics typically have abundant data parallelism which can be exploited.
Such applications often require shared data structures to store data itself (such as a graph), collect intermediate results, and to support the underlying runtime. Because of the massive number of parallel threads needed for latency tolerance, these shared, distributed data structures are a source of significant contention. Naive locking strategies, even fine-grained, resulted in excessive serialization, preventing these data structures from being used as intended.

*Combining* [@flatCombining;@yew:combining-trees;@funnels] is a technique which can reduce contention on shared data by distributing synchronization. Basically, combining exploits the *associativity* of some ADT operations which can be merged together into a larger aggregate operation *before* being applied to the shared data structure. For example, individual `Set.add` operations can be combined into a single operation that adds multiple elements. Doing so moves some of the synchronization off of the hot data structure – now several separate synchronizations are just one. This is useful because combining can be done in parallel on many different threads. In some situations, operations even *annihilate* one another – that is, they cancel each other out, as is the case with a `push` and `pop` to a stack – which eliminates any need for global coordination of those particular operations.

Combining has been used in many different shared-memory systems to reduce contention and data movement. In a similar way, MapReduce allows a *combiner* to be defined to lift part of the reducer's work into the mapper [@Dean:08:MapReduce]. We applied the concept to Grappa's distributed shared data structures and observed significant performance improvements [@flat-combining-pgas13]. In the distributed setting, combining can be even more effective as local synchronization within a node can eliminate many costly round-trip communications to other nodes. We also developed an extensible framework for developing data structures with combining for Grappa applications.

## Claret: abstract data types for high-contention transactions
One of the most popular key/value stores in use today is Redis [@redis], which is special in that it supports a much wider range of complex data types and many operations specific to each type. However, Redis does not support general distributed transactions because they are considered too expensive.
We observed that by treating Redis's data types as ADTs, we could expose significantly more concurrency between transactions to make them practical even for high-contention workloads. One technique crucial to this is *abstract locks*.

### Abstract locks {#sec-abstract-locks}
~ Tab { #tab-spec caption="Abstract Commutativity Specification for Set." }
~~ Center
| Method             | Commutes with   | When                        |
|:-------------------|:---------------|:----------------------------|
|`add(x): void`      | `add(y)`       | $\forall x, y$              |
|--------------------|----------------|-----------------------------|
|`remove(x): void`   | `remove(y)`    | $\forall x, y$              |
|                    | `add(y)`       | $x \ne y$                   |
|--------------------|----------------|-----------------------------|
|`size(): int`       | `add(x)`       | $x \in Set$                 |
|                    | `remove(x)`    | $x \notin Set$              |
|--------------------|----------------|-----------------------------|
|`contains(x): bool` | `add(y)`       | $x \ne y \lor y \in Set$    |
|                    | `remove(y)`    | $x \ne y \lor y \notin Set$ |
|                    | `size()`       | $\forall x$                 |
|--------------------|----------------|-----------------------------|
~~
~

Databases commonly use *reader/writer locks* to control access to records in conjunction with a protocol such as two-phase locking to ensure isolation between transactions.
With reader/writer locks, multiple readers can hold the lock at the same time because they do not modify it, but anyone wishing to perform mutation must hold an *exclusive* writer lock.
*Abstract locks* [@Schwarz:84:ADT;@Weihl:88:ADT;@Herlihy:88;@Badrinath:92;@Chrysanthis:91] generalize this notion to any operations which can logically run concurrently on the same object.
Abstract locks are defined for a particular ADT in terms of a *commutativity specification* which describes with pairs of operations commute with one another: a function of the methods, arguments, return values, and abstract state of their target. An example specification for a `Set` is shown in [#tab-spec].

When used in the context of transactions (termed *transaction boosting*), abstract locks can drastically reduce conflicts by allowing more operations to execute concurrently on the same record [@Herlihy:PPoPP08]. This is particularly crucial for highly contended records, where the chances of having concurrent operations is high, and serializing operations can become a bottleneck. In essence, abstract locks allow transactions to overlap more, only serializing when they absolutely must in order to ensure they cannot observe inconsistent state.

### Claret
Our prototype ADT-store, *Claret*, has a similar programming model to Redis
Underneath the abstraction afforded by the ADTs, Claret implements abstract locks, combining, and a form of lock reordering called phasing. On three transactional workloads simulating realistic contention – a microbenchmark similar to YCSB+T [@YCSB-T], an online auction service, and a Twitter-like social network – Claret achieved a 3-50x speedup over naive transactions and within 67-82% of the performance without transactions.

## Reveling in the bounty of inaccuracy

The ADTs used in Grappa and Claret exposed concurrency without sacrificing safety or correctness. This imposed a limit to the amount of concurrency they could exploit.
We compared the performance of Claret's transactions against the same workloads without transactions. Though Claret's transactions were competitive, they fundamentally could not out-perform the non-transactional workload because they could not allow conflicting operations to be executed concurrently. For example, no matter how much commutativity abstract locks could expose, `Bid` and `ViewAuction` transactions had to be separated because `ViewAuction` required viewing the current maximum bid. What if we could relax this requirement and allow clients to view inaccurate results? What can be done to make this as safe as possible?

As we have discussed throughout this paper, there are significant performance benefits to be had by relaxing consistency and exposing weak replication. To give developers full control over these opportunities, we must allow them to trade off consistency in their applications. The next project proposes to do just that by using a new class of ADTs.

# Disciplined Inconsistency {#sec-disciplined}
Our goal is to come up with a programming model that helps programmers balance all of these competing requirements; ideally, it should have the following properties:

- Minimize unnecessary constraints by exposing safe concurrency
- Express where and what errors can be tolerated, so consistency can be traded for performance.
- Communicate performance requirements such as target latency or availability.
- Easy to reason about and modular

At this point we have established that trading off consistency for performance is tricky business, involving making many decisions about what reorderings of operations should be allowed, when updates must be visible in order to ensure correct execution, or how consistent a read can be and still meet its latency SLA. Furthermore, programmers must make these decisions while keeping in mind that due to real-world effects, some data items will be significantly more contentious and inconsistent than others.
The promise of ADTs is to hide implementation details – can we use them to hide some of these concerns? 

With *inconsistent, probabilistic and approximate (IPA) types*, we can express weaker constraints on ADTs, unlocking the possibility of using the techniques we have discussed to trade off consistency for performance.
Operations on IPA types only allow views of the state that can assure correct semantics, which may mean disallowing operations that would expose too much, or exposing values that encode a range or distribution rather than a precise value.
IPA types encapsulate the ordering and visibility constraints necessary for whatever operations they allow, so users do not need to interact with complex consistency models, and these types can subsequently be composed. Of course, it leaves programmers with a different set of problems to deal with, which we will get to, but which we posit are an better alternative.

Let us consider again the scenario posed at the beginning, about a ticket sales app that failed to handle the load when the new Star Wars movie came out. Its programmers likely had several requirements in mind for the app's behavior:

1. Do not sell more tickets than are available.
2. Allow users to view an *estimate* of how many tickets remain when they load the page.
3. Initial page load must have a 99th percentile latency of 100 ms.

If we allow two replicas to both sell the last ticket, we will violate the first requirement, so our model must express this as a hard constraint. Using *escrow*, a technique we will explain next, we can distribute permissions to sell tickets among the replicas so that while there are many tickets remaining, the requests can be handled by any replica safely. However, now providing a precise count of remaining tickets requires synchronizing all replicas. Luckily, (2) tells us that the count does not need to be precise, which allows us to meet (3)'s requirement by accessing whichever replica is nearest or fastest.

~ Fig {#fig-tickets caption="A `MovieTicket` ADT can be implemented using a more generic `Pool` type. Ticket sales must be strongly consistent, but the number of remaining tickets can be approximate, provided it gets more precise when there are few remaining." }
![ticket sales example](fig/tickets.pdf)
~

[#fig-tickets] shows a possible way of specifying this movie ticket sale as an IPA type. In this example, we derive our `MovieTickets` type from a generic `Pool` that ensures `take` operations are unique and upper-bounded by the number of items in the pool, and supports approximate size. 
Without some form of bound, the approximate size is somewhat underspecified. In this case the application's SLA requests a bounded latency but a best-effort value, so the latency bound serves to determine how the size will be obtained and how approximate it will be. Alternatively, we could imagine a different application wishing to instead specify a target value bound, such as a maximum 10% error. 
We address this duality between performance and precision next.

## Duel of duals
Consistency and performance are coupled – more consistency requires coordination which costs performance; achieving performance targets may require sacrificing consistency.
We have covered many techniques that acknowledge this tradeoff and allow applications either to specify weaker consistency in order gain performance or specify coordination requirements and give up some performance. However, only Pileus's consistency-based SLAs [@Terry:13:SLAs] made performance targets explicit and provided *feedback* to programs about the consistency achieved. 

Throughout this paper we have seen that distributed systems require give and take: more consistency requires more coordination and costs performance, achieving performance goals requires sacrificing consistency or precision.

It is common today for the specifications for applications to include *performance bounds*, such as target latencies specified by an SLA, or the requirement that a service be *highly available*. More strict performance bounds imply more uncertainty in terms of values – consistency is weaker, reads are forced to take whatever they can get, even if it is stale. Because it is not surfaced explicitly, this increase in uncertainty can go unaddressed, leading to noticeable consistency issues later. Instead, can we make these uncertainties explicit in the programming model, ensuring programmers handle them correctly?

In other situations, perhaps only a certain amount of error can be tolerated. For example, when viewing a tweet with only a few retweets, one would expect the count to be exact, because being off by even 1 would be obvious. However, when viewing a super popular tweet, like a Justin Bieber selfie, the number of retweets could be in the millions, so it can be off by thousands. In these situations, it would be useful to be able to specify an *error tolerance*, such as a 10% tolerance on the count. As the dual of performance-bounded operations, perhaps the programming model ought to provide estimates or analysis of the *performance uncertainty* for operations with bounded error. Error bounds will also typically need to come with some form of *confidence level*.

In our work, we aim to provide all of these as options so that no matter the situation, programmers have the tools they need to make those tradeoffs.

As an aside, the goals of BlinkDB [@Agarwal:13:BlinkDB] bear a lot of similarity to ours, except they trade off the amount of data touched rather than consistency. In BlinkDB, SQL queries with aggregates (such as `count` or `average`) are annotated with either *time bounds* or *error bounds*, and the database uses sampling to get the best answer it can within the bounds. Their approach requires partial knowledge of the distribution and maintenance of "stratified samples" in order to be able to estimate the error with confidence. We will need different approaches to estimate error under weak consistency.

## Programming model
What kinds of approximations do we need to be able to express? How do they manifest themselves as ADTs? In the *Disciplined Inconsistency* programming model, bounds are defined on ADTs, and operations return *IPA types*.

### Specifying bounds on ADTs
There are several forms of constraints which we will aim to support. These include some of the constraints supported by Indigo [@Balegas:15:Indigo] that are compatible with ADTs, extended with new hard and *soft* constraints. We expect bounds to typically be expressed on application-specific ADTs, as in [#fig-tickets] and [#fig-retwis].

#### Numeric constraints
These can apply either to simple standalone numeric values, or more commonly to integer quantities associated with data structures, such as the cardinality of a Set. Constraints can be hard upper or lower bounds, or a *tolerance* of some distance from the precise value (such as the 10% tolerance we mentioned earlier). *Uniqueness*, a common desirable constraint [@Balegas:15:Indigo;@Bailis:15:Feral], is a degenerate case with an upper bound of 1.

#### Filter or membership constraints.
[todo: ???]

### IPA Types
Most operations with a performance or value bound will return a value with some form of uncertain type, depending on how the bound is fulfilled. These types fall into 3 different categories: *inconsistent*, *probabilistic*, or *approximate*.

#### Approximate types.
These types are the simplest for programmers to use, and should be preferred. Approximate types encode the set of all possible correct values. Note that in situations of replication, there is likely no single globally *correct* value – different replicas can hold different values simultaneously, and there could be any number of staged operations whose final commit order is yet unknown. 
Example approximate types include `Interval<T>`, which specifies an upper and lower bound on possible types. Any type with a defined *partial order over values* (a lattice) can be represented as an interval: numeric types are obvious, but a *set*, for example, could have an interval defined by a number of items that may or may not be in the set. An interval could also be defined as a *point* and *radius* – for example, a mean &plusmn; 5% – depending on the desired semantics.

#### Probabilistic types.
In some situations, guaranteeing the hard bounds encoded by approximate types is too expensive. Bounds can be weakened by providing a probabilistic guarantee instead. These typically take the form of a distribution, such as a gaussian defined by a mean and standard deviation, with a certain confidence level. For example, PBS's models do not ensure hard bounds on staleness but rather a probability distribution that defines whether it is correct or not. We expect programmers to use these types for simple inference questions, such as "Is the value greater than 100 with 95% confidence?", in order to determine if something should be displayed.
This is closely related to how `Uncertain<T>` [@Bornholt:14:UncertainT] allows programmers to reason about uncertain values coming from sensors.

#### Inconsistent types.
Finally, in the worst cases, there may be no way to bound the potential values. For instance, any technique providing hard bounds may need to limit the rate at which updates are applied to replicas. If absolute maximum availability or throughput is required, then those restrictions cannot be applied. However, we can still help programmers be *disciplined* about these unsafe cases using *inconsistent types*. 

These types are the most opaque. At most, they may provide a measure of staleness, such as a `Stale<T>`, which would allow users to know how out of date the value is, and potentially surface this to the user. An example of this is shown in [#fig-retwis]. However, even a nearly completely opaque `Inconsistent<T>` can help protect inconsistent values from accidentally flowing into consistent computations, similar to how *taint analysis* prevents secure values from being leaked.

## Implementation
The implementation of Disciplined Inconsistency involves developing the frontend programming model based on ADTs with additional bounds, and integrating it with a backend *enforcement system* which integrates many of the prior techniques covered in this document.

### Enforcement system
The prior techniques discussed in this document provide ample ways to enforce invariants and perform efficient coordination. We will focus primarily on *escrow*, *reservations*, and *leases*, integrated with ideas from *abstract locks* to implement most of the coordination required for *approximate types*. Indigo [@Balegas:15:Indigo] demonstrated one way of combining abstract locks ("multi-level locks" in their terminology) with reservations to implement the wide range of constraints they supported. For instance, numeric constraints can be implemented using *escrow* to distribute permissions among all the replicas. For example, to implement our `Pool` type from [#fig-tickets], we could create an escrow *piece* for each ticket ahead of time, then distribute those among the replicas. When clients execute a `take`, a piece is allocated from the pool of remaining operations. If the `take` aborts before finishing, the piece is simply returned to the pool. Likewise if a replica goes offline, the pieces are not actually lost – they can be reclaimed after a period of time and re-used.

Another component of the enforcement system will behave similarly to PBS and Pileus's consistency SLA system, to provide *probabilistic* uncertainty. In these systems, hard guarantees are not enforced as with escrow reservations. Instead, this subsystem monitors various health metrics about the system: replication latency, write load, etc. These factors can then be fed into simple predictive models to make judgements about the probability that a given value is being updated currently.

Largely unmodified CRDTs provide the basis for data type implementations that do not use reservations to prevent conflicts. Operations on these types should return some form of IPA type – by default, just an `Inconsistent<T>`, but if version vectors or other staleness information is available, more expressive IPA types can be used.

### Implementation strategy
As a first pass, the programming model will simply be hand-written data types supporting a small number of configurable latency and value bounds. These data types will at first manually specify how they are to be enforced using components from the enforcement system described above.

Once we have established the potential using manual implementation, we will investigate ways of automatically generating the correct enforcement given specified latency bounds and IPA types. Another avenue of potential investigation is to see if the bounds annotations can be used to infer the correct IPA types, or at least do static type checking to ensure that the interface is enforceable with the desired bounds.

## Case studies
### Twitter
~ Fig {#fig-retwis .zoom caption="*Twitter clone:* Loading a user's timeline with IPA types."}
~~ Center
![](fig/retwis.pdf){max-height=20em}
~~
~

Twitter is subject to all kinds of extreme contention resulting from realtime events, and power law effects. Famously, early in Twitter's lifetime, it would go down any time traffic spiked, such as during World Cup goals; each time showing the now famous Fail Whale [@failwhale]. Even late in its life, Twitter was slowed to a standstill at the 2014 Oscars when Ellen Degeneres tweeted a selfie with several celebrities which was retweeted at record-breaking speed.
Luckily, there are many aspects of Twitter that are amenable to inconsistency. In fact, most of the real Twitter application is served out of their eventually consistent datastore, Manhattan [@manhattan]. However, if we can quantify specific places where accuracy is unnecessary, perhaps we will not need to give up consistency everywhere else.

**Followers, retweets, favorites counts.** As mentioned before, for an extremely popular tweet, these counts are truncated – Justin Bieber's profile lists that he has "69.8M" followers rather than the precise value, because it is constantly changing (but not often by more than 100,000...), and his most recent tweet was retweeted "20K" times. On the other hand, most tweets have few favorites, and if those counts are off they will be noticed. This is a perfect place for a tolerance-based constraint that will scale with the size of the count.

**Missing tweets.** Another possible relaxation is which tweets appear in a timeline. If a timeline is missing a recent tweet, odds are good the user will not be able to tell. This may be a situation where a performance bound is called for – no one wants to wait a long time for their timeline. This could then return a measure of the *staleness* of the timeline, such as the latest point in time before which tweets are guaranteed to be included.

For Claret, we implemented a simplified Twitter clone based on a Redis application called Retwis [@retwis]. This application will be extended to use disciplined inconsistency to further improve performance and availability.

### Auction
Auction services are generally considered a class of applications requiring strong consistency. Finding the correct maximum bid, in spite of any amount of traffic, is crucial for fairness. However, they also have significant contention problems as some auctions receive far more bids than the average (they follow a power law), and bidding ramps up right before the auction closes. Allowing as many bids as possible during that late stage is crucial to keeping users happy and maximizing revenue. We base our implementation on the functionality of Rubis [@Amza:02:Rubis].

**Current high bid.** While an auction is ongoing, users typically want to know what the going rate is so they can decide if they are willing to over-bid. If this is changing frequently, the high bid is going to change rapidly. At that point, users are unable to really tell what the current bid will be when they actually make their own bid. Therefore, they probably do not need to see the most precise version of the bid. Some estimate of the current maximum bid, provided it is reasonably accurate, is acceptable because the true high bid can still be found later. This is an ideal situation for a latency-bound operation that returns a range of possible values or a probability distribution.

**Stale listings.** Rubis allows clients to browse currently open auctions by region or category. In this view, one could imagine wanting to show an estimate of the price and other relevant information about the auctions. However, it could be prohibitively expensive to get accurate, up-to-date bid information. Furthermore, in large deployments, popular categories could have many new auctions opening constantly. These both seem like situations where a stale version, either of the list of current auctions, or the current bids for each auction, would be acceptable.

### Ticket Sales
We have used the movie ticket sales example throughout this paper, but to reiterate, there are a number of interesting challenges in supporting ticket sales. The movie ticket example is a bit contrived because no one theater will have all that many seats available, so contention will be distributed even for popular movies. However, ticket sales for other events with much larger venues can experience serious problems with contention. We already discussed how the *remaining tickets* field is a candidate for approximation.

FusionTicket [@FusionTicket] is an open source web application for selling tickets to events that has been used in some recent transactions research as a benchmark [@Xie:14:Salt;@Xie:15:ACIDAlt]. We could use this benchmark as a starting point and look for additional places where contention is a problem and where error can be tolerated.

### Streaming analytics
Trending topics, realtime recommendations, and performance monitoring are just a few examples of the kinds of analytics services like Twitter or Facebook perform continuously. These features can be crucial to user engagement. However, they can also typically be treated as a best-effort or "nice to have" addition to the experience; whenever performance becomes a problem, these features can be dialed back. Furthermore, they are typically informed by machine learning algorithms which introduce significant noise. Therefore, they are prime candidates for disciplined inconsistency.

One concrete example: Twitter's revamped streaming analytics platform, Heron [@Kulkarni:15:Heron], supports a feature called *backpressure*. When a downstream processing element becomes overloaded and is unable to keep up with the rate of incoming data, it will fall behind and no longer be *realtime*. Backpressure tells upstream data generators to dial back their output, typically done by sampling, until the backlog is under control and processing returns to normal. This can cause discontinuities in the output of analytics which may persist if programmers do not handle it correctly. Perhaps some form of probabilistic data type could be used to communicate the sampling rate of data items, making it simpler for programmers to handle these cases.