# Techniques for avoiding contention

So far, we have covered a number of programming models which helped mitigate contention by replicating data and weakening consistency. Now we will discuss some techniques that can be used to implement those programming models and actually expose concurrency within distributed systems. The goal of each of these techniques is to allow concurrency wherever it is safe and minimize the coordination necessary to accomplish it.
<!-- Most of these techniques were designed to enforce strict correctness, such as strict commutativity among operations on an ADT. However, they can be co-opted to help enforce soft constraints as well.  -->

## Combining {#sec-combining}
Some operations on ADTs can be merged together *before* being applied to the object.
This technique, known as *combining* [@flatCombining;@yew:combining-trees;@funnels] can drastically reduce contention on shared data structures and improve performance in situations where applying the combined operation is cheaper than applying the operations one-by-one. In distributed settings, combining can be even more useful as it effectively distributes synchronization for a single data structure over multiple hosts [@flat-combining-pgas13].

Basically, combining exploits the *associativity* property of some operations to do part of the computation early, in parallel, before performing them on a shared data structure, where operations are serialized. Combining has been used in many different shared-memory systems to reduce contention and data movement; even MapReduce [@Dean:08:MapReduce] allows a *combiner* to be defined to lift part of the reducer's work into the mapper.

## Abstract locks {#sec-abstract-locks}

~ Tab { #tab-spec caption="Abstract Commutativity Specification for Set." }
| Method             | Commute with   | When                        |
|:-------------------|:---------------|:----------------------------|
|`add(x): void`      | `add(y)`       | $\forall x, y$              |
|--------------------|----------------|-----------------------------|
|`remove(x): void`   | `remove(y)`    | $\forall x, y$              |
|                    | `add(y)`       | $x \ne y$                   |
|--------------------|----------------|-----------------------------|
|`size(): int`       | `add(x)`       | $x \in Set$                 |
|                    | `remove(x)`    | $x \notin Set$              |
|--------------------|----------------|-----------------------------|
|`contains(x): bool` | `add(y)`       | $x \ne y \lor y \in Set$    |
|                    | `remove(y)`    | $x \ne y \lor y \notin Set$ |
|                    | `size()`       | $\forall x$                 |
|--------------------|----------------|-----------------------------|
~

Locks ensure mutual exclusion between conflicting operations. They can be used at various granularities, but a typical configuration is to associate a lock with each shared object or data structure in multithreaded programs, or with each *record* in datastores. 
The simplest of locks allow only a single holder at a time, but a common extension is to allow multiple readers to hold the lock at the same time, known as a *reader/writer (r/w) lock*. These ensure that operations that may modify the state will not interfere with each other, but enables parallel access for non-mutators.

*Abstract locks* [@Schwarz:84:ADT;@Weihl:88:ADT;@Herlihy:88;@Badrinath:92;@Chrysanthis:91] generalize the notion of reader/writer locks to any operations which can logically run concurrently on the same object.
Abstract locks are defined for a particular ADT in terms of a *commutativity specification* which describes with pairs of operations commute with one another: a function of the methods, arguments, return values, and abstract state of their target. An example specification for a `Set` is shown in [#tab-spec].

When used in the context of transactions (termed *transaction boosting*), abstract locks can drastically reduce conflicts by allowing more operations to execute concurrently on the same record [@Herlihy:PPoPP08]. This is particularly crucial for highly contended records, where the chances of having concurrent operations is high, and serializing operations can become a bottleneck. In essence, abstract locks allow transactions to overlap more, only serializing when they absolutely must in order to ensure they cannot observe inconsistent state.

Provided a correct commutativity specification, abstract locks ensure linearizability — concurrent updates are only allowed when it is impossible to tell if they have been reordered. They are not designed for weakly consistent replicated data. For that, we must turn to the Law for inspiration.

## Escrow and Reservations {#sec-escrow}
*Escrow* is a term from banking and legal proceedings where some amount of money is set aside and held by a third party in order to ensure it will be available for use at a later time after some (typically legal) condition is met. In database systems, this term has been borrowed for use in concurrency control to refer to "setting aside" some part of a record to be later committed.

O'Neil's idea of *escrow* [@ONeil:86] came from work on Fast Path [@Gawlick:85] and Reuter's Transactional Method [@Reuter:82]. The idea was to increase concurrency on *aggregate fields*, such as fields keeping track of a count or a sum, which could become hot spots because they were updated frequently. The idea of *escrow* is to split up an aggregate value into a *pool* of partial values and allocate parts from the pool to transactions when they execute so that when they are ready to commit, they are guaranteed to be able to. For example, if a transaction is going to decrement an account balance as long there are sufficient funds, it will hold the amount it wishes to decrement *in escrow*. Other transactions can also decrement the balance, provided combined they will not leave the balance negative. If a transaction aborts, the escrowed values are returned to the pool.

Escrow can be extended to any *fragmentable object* [@Walborn:95], that is, any data type providing a way to *split* itself into fragments of the same type, and a way to later *merge* the fragments back together. This is essentially the same criteria as for *monoids* used for aggregators in Summingbird [@summingbird], although used in the opposite way, and similar to the isolation types in Concurrent Revisions [#sec-revisions].

*Reservations* [@Preguica:03] or *warranties* [@Liu:14:Warranties] can be thought of as escrow for replicated data. A reservation pre-allocates permission to do updates so that in the future they can be done without coordination. This moves coordination off the critical path and allows it to be amortized. For example, in Mobisnap [@Preguica:03] where they were first introduced, a salesman might reserve a number of tickets or a some quantity of a commodity, then while on the go, without connectivity, make a sale and know that it is safe to do so. Reservations are very easy to generalize — Indigo [@Balegas:15:Indigo] uses them to implement its application-specific conflict avoidance logic that includes auto-generated code.

## Leases {#sec-leases}
The problem with datastore reads is that by the time the client gets the result, it could already be out of date. Leases are a way of communicating how old a read is, by associating it with a time in the future when it should be considered stale.
First proposed for file system caches to avoid needing to send explicit invalidations [@Gray:89], they are now used in application caches in modern datacenters, such as in Facebook's Memcache system [@Nishtala:13:Memcache]. In addition to simply bounding staleness without explicit invalidation, leases can be used to indicate a promise that the value will not be updated for some time.

Leases can be combined with reservations to grant permission to perform updates for a fixed amount of time [@Shrira:08]. This has the benefit of allowing reservations to be reclaimed if a client crashes, or saving a return message if a recipient does not wish to use the reservation.

## Transactions
[todo: this doesn't make sense] Programmers do not need to give up all hope, though: there are several techniques for exposing additional concurrency within transactions to overcome this hurdle.

### Chopping and chaining
This old technique [@Shasha:95] automates what programmers could do by hand: break up transactions into the minimal-sized pieces which must execute atomically, so that locks can be held only as long as absolutely necessary. This involves static analysis to find interleavings which must be disallowed and requires a mechanism for chaining transactions together so all the effects can be un-done on a later abort. 

A more recent system, Lynx [@Zhang:13:Lynx], splits up transactions by shard and executes them as a *chain*, moving from shard to shard and coordinating execution order to allow conflicting transactions to interleave safely. Additionally, they statically find operations that are commutative and use the knowledge to avoid over-coordination.

Callas [@Xie:15:ACIDAlt] takes this notion further 
split transactions into groups and allow them to be concurrent [@Xie:15:ACIDAlt]

### Boosting


