# Introduction {#intro}

<!-- Imagine a young web app, bright-eyed and ready to face the world with a little local datastore to make sure it doesn't forget a moment of this exciting day. It reaches its thousandth customer and now that local datastore feels a bit cramped, so it swaps in a distributed key/value store that can grow with it. Then one day, it finally gets noticed — featured on TechCrunch and on the top of HackerNews — and suddenly it's inundated with people excited to try it out. The key/value store scales out to handle all the new data and users. But there is a problem: a couple nodes are receiving far more traffic than the rest. It turns out that Barack Obama just joined and everyone wants to see what he's doing. Other hot spots spring up here and there, whenever something important happens in the news or a meme goes viral, and each time the hot nodes get bogged down, latencies spike, and users get frustrated. -->

Imagine you are a young ticket selling app, embarking on a mission to help people everywhere get a seat to watch their favorite shows. Bright-eyed and ready to face the world, you store everything in a key/value store so that you will not forget a moment of this exciting time. It's a distributed key/value store, so when you expand into new cities and reach your millionth customer, your datastore continues to grow with you. You enable strong consistency and use transactions to ensure that no tickets are sold twice or lost, and your customers praise your reliability. "That little app has never steered me wrong", they say.

Then one day, pre-sales for the 7th Star Wars movie come out, almost 40 years after the original, and suddenly you are inundated under a surge over 7 times your usual load, as a record-breaking number of people try to purchase tickets for this one movie. This concentrated traffic causes *hot spots* in your datastore; while most nodes are handling typical traffic, the traffic spike ends up being funneled to a handful of nodes which are responsible for this movie.
These hotspots overload your poor datastore, causing latencies to spike, users' connections to time out, and you have disappointed users who will not get to see the movie on opening night because you were not able to sell them a ticket.
Even major players like Fandango, Regal, and AMC are plagued with service interruptions; some sites even crash or lose data.

This kind of concentrated surge in traffic is a perfect illustration of the contention that occurs all the time in real-world workloads, from ticket sales to social networks. How do applications cope with this challenging workload? Many techniques over the many years have tackled this problem from different angles, from research on escrow and fragmenting aggregate fields in the 80s, to modern research on compiler-generated coordination logic from invariant annotations. Many techniques require a variety of changes to the programming model, while others improve underlying protocols and mechanisms. All are focused on exposing concurrency and making tasks simpler for programmers.
This paper will explore the various dimensions upon which these techniques operate to expose concurrency and improve programmability and, using what we have learned from this prior work, propose a way to bring many of the techniques together under one simple and old abstraction: abstract data types.

## Mitigating contention

~Fig { #fig-hotspot caption="Overview of approaches for mitigating contention." }
![](fig/hotspot.pdf)
~

Mitigating contention is about exposing concurrency. At the highest level, this can broken down into a few different broad approaches, shown in [#fig-hotspot].

First, is there any concurrency on the contended record which can be exposed within this shard of the datastore? If operations on the record are commutative, then they can safely run concurrently without changing the semantics; *abstract locks* ([#sec-abstract-locks]) leverage this to allow transactions to overlap and avoid false conflicts. *Escrow* ([#sec-escrow]) allows the record to be treated as if it was split into a pool of fragments.

Second, if clients are multithreaded, as is the case with frontend web servers that typically handle many end-user requests concurrently, then some of the synchronization work can be offloaded to them. *Combining* ([#sec-combining]) leverages associativity to allow operations to be merged locally and executed as a single operation remotely, reducing the amount of work done on the overloaded datastore. Other techniques like *leases* ([#sec-leases]) let client-side caches avoid costly invalidation messages.

Third, the load on the contended shard can be reduced by allowing clients to interact directly with replicas rather than funneling all writes through the master as is done in strongly consistent systems.
However, this comes at a significant programmability cost: the illusion of a single copy of data is broken and programmers must now reason about replicated state.
Replicas typically share updates among each other asynchronously, and clients may communicate with multiple replicas, so they can observe effects out of order, or perform updates which conflict and result in inconsistent states.

The plethora of weak consistency models, such as eventual consistency or causal consistency, exist to provide more reasonable programming models for working with such replicated state. And the massive body of research aimed at reigning in those models attests to their weaknesses and the difficulty programmers have in using them.


In this work we will explore many ways of mitigating contention. 
These problems and their solutions can be broken down along several axes:

- Ordering constraints between operations
- When and who updates become visible to
- Uncertainty about the state in terms of staleness and possible values
- Granularity: does it affect the whole system, specific records, or specific operations?



## Concurrency

Contention comes about *because* of concurrency. Lots of concurrent threads trying to access the shared resource.

At the same time, concurrency is also our best tool to fight contention. We must find ways to increase concurrency *within the contended resource*, to allow more to use it at the same time.

The trick is to find where this concurrency exists in applications.

Eventually, there is no more strict concurrency to be exploited. The only way forward is to relax correctness. That's where replication comes in...

## Replication

Replication is done for many reasons: performance (introduce parallelism), reduce latency, fault tolerance, or availability.

Geo-replication, mobile, and in-memory caches (memcached, Redis) are all examples of *replicated data*.

We can't afford strict serializability, which would make them all appear as a single "machine", as we are used to in multi-core shared memory thanks to the efficiency of cache coherence. [todo: why not? why is it too expensive? (because of coordination? ordering constraints? — does cache coherence deal with order?)]

Instead, we must deal with replicated data explicitly. Traditionally, we would do this with weak consistency models, analogous to weak memory models in computer architecture.

Trade off consistency for performance, availability, etc.

## Constraints {#constraints}

Distributed applications have many competing requirements. At the very least, they are expected to be *scalable*, *fault tolerant*, and *highly available*. No matter how many users are active or where they are, they must be able to access the application and get something useful out of it.

Competing with these desired properties is the need for *correctness* — an indulgence, to be sure, but some people seem to think they want it. Strong consistency, strict serializability, and ACID transactions are among the many useful programming tools that must be sacrificed to achieve the previous properties.

Luckily, not everything requires the same level of precision or consistency. There are some actions, such as choosing the winning bid in an auction, or selling the last ticket, where exact correctness is required. However, there are many other situations, such as viewing recent popular tweets or trending hashtags, where users are unlikely to care or even know if the results are imprecise or stale. Even in the auction example, individuals glimpsing an auction in progress may tolerate a somewhat low estimate of the current top bid. While some of these constraints are *soft*, it is still desirable to be as close to correct as possible and there may be some threshold where incorrectness becomes intolerable. For instance, it is in the auction service's best interest to show the most up-to-date top bid in order to get the highest price and maximize the auction service's revenue and customer satisfaction.

At the same time, there are hard and soft constraints on performance. Many services have service-level agreements (SLAs) promising a response within a given latency for all but the 99.9th percentile of requests. Hip startups must be able to handle all the requests from their exponentially growing user bases.

Programmers of these distributed applications must constantly juggle these competing hard and soft correctness and performance constraints. There are a huge variety of techniques that have been invented over the years for handling them: new concurrency control schemes, programming models, and data representations, to name a few. Each has its own trade-offs, requiring varying degrees of application changes support them. There ought to be an abstraction that can help programmers make these competing trade-offs and inherit the body of existing optimization techniques.

## Programming model wish list

- Easy to reason about (composable, *meets programmer expectations*)
- Minimize unnecessary constraints, expose application-level concurrency
- Allow trading off consistency for performance (latency/availability)

## Abstract data types: a new old programming model

Abstract data types (ADTs), one of the oldest [todo: ?] abstractions of Computer Science, provide the core of our solution to the problem. ADTs expose a set of *logical operations* over *abstract state* of a data type while encapsulating the details of their implementation. This makes it easy for programmers and systems alike to reason about their behavior. This ease of reasoning extends to multiple data types, as ADTs can be composed in a variety of ways. Part of an ADT's specification is which operations are commutative or associative or monotonic with respect to the current state and each other, so they naturally express concurrency and ordering constraints, which systems can take advantage of. Finally, ADTs can be used to trade off precision as well by exposing inconsistency in the form of new data types and operations with error bounds, probability distributions, or other semantic approximations.

This paper will explore the many existing ways of mitigating contention and managing weakly consistent replicated data and how ADTs can include all of them.
[todo: ...]
