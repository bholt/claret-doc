# Introduction {#sec-intro}

<!-- Imagine a young web app, bright-eyed and ready to face the world with a little local datastore to make sure it doesn't forget a moment of this exciting day. It reaches its thousandth customer and now that local datastore feels a bit cramped, so it swaps in a distributed key/value store that can grow with it. Then one day, it finally gets noticed — featured on TechCrunch and on the top of HackerNews — and suddenly it's inundated with people excited to try it out. The key/value store scales out to handle all the new data and users. But there is a problem: a couple nodes are receiving far more traffic than the rest. It turns out that Barack Obama just joined and everyone wants to see what he's doing. Other hot spots spring up here and there, whenever something important happens in the news or a meme goes viral, and each time the hot nodes get bogged down, latencies spike, and users get frustrated. -->

Imagine a young ticket selling app, embarking on a mission to help people everywhere get a seat to watch their favorite shows. Bright-eyed and ready to face the world, it stores everything in a key/value store so that it will not forget a thing. It uses a distributed key/value store, so when it expands into new cities and reaches its millionth customer, the datastore continues to grow with it. The app uses strong consistency and transactions to ensure that no tickets are sold twice or lost, and its customers praise its reliability. "Five stars. That little app always gets my tickets, fast!" they say.

Then one day, pre-sales for the 7th Star Wars movie come out and suddenly it is inundated under a surge over 7 times the usual load, as a record-breaking number of people try to purchase tickets for this one movie. This concentrated traffic causes *hot spots* in the datastore; while most nodes are handling typical traffic, the traffic spike ends up being funneled to a handful of nodes which are responsible for this movie.
These hotspots overload this app's "scalable" datastore, causing latencies to spike, users' connections to time out, and disappointed users who will not get to see the movie on opening night because the app was not able to sell them a ticket.
On this dark night for moviegoers, even major players like Fandango, Regal, and AMC are plagued with service interruptions; some sites even crash or lose data.

This kind of concentrated surge in traffic is a perfect illustration of the contention that occurs all the time in real-world workloads, from ticket sales to social networks.
Power law distributions are everywhere, from popularity of pages to number of followers, leading to network effects which can magnify even the slightest signals. Events – such as the FIFA World Cup or the Oscars – happening in realtime drive spikes in traffic. All together, this can create memes that propagate virally through social media and news sites. Interactivity is crucial to all of this — it both fuels the propagation of memes and causes contention.

Paradoxically, though contention is not the average case, it is responsible for many of the most challenging problems: tail latency, failures, and inconsistency bugs.
Most of the time, an application may work correctly, with low latency and no noticeable inconsistency. However, in that 99th percentile case, contention results in a hot spot, where latencies spike, failure rates go up, and consistency can degrade, exposing bugs or resulting in user-visible inconsistencies.

## Mitigating contention {#sec-hotspot}
Many techniques, over many years, have tackled this problem from different angles, from research on escrow and fragmenting aggregate fields in the 80s [@ONeil:86], to modern research on auto-generated coordination based on annotations [@Balegas:15:Indigo;@Sivaramakrishnan:15:Quelea]. Some require a variety of changes to the programming model, while others improve underlying protocols and mechanisms. All are focused on exposing concurrency and making tasks simpler for programmers, but they can be broken down into three broad approaches, shown in [#fig-hotspot].

<!-- This paper will explore the various dimensions upon which these techniques operate to expose concurrency and improve programmability and, using what we have learned from this prior work, propose a way to bring many of the techniques together under one simple and old abstraction: abstract data types. -->

~Fig { #fig-hotspot caption="Overview of approaches for mitigating contention, such as the hotspot in red." }
![](fig/hotspot.pdf)
~

1. Is there any concurrency within the contended record that can be exposed? If operations on the record are commutative, they can safely run concurrently without changing the semantics; *abstract locks* ([#sec-abstract-locks]) leverage this to allow transactions to overlap and avoid false conflicts. *Escrow* ([#sec-escrow]) allows the record to be treated as if it was split into a pool of fragments.

2. If clients are multithreaded, as is the case with frontend web servers that typically handle many end-user requests concurrently, then some of the synchronization work can be offloaded to them. *Combining* ([#sec-combining]) leverages associativity to allow operations to be merged locally and executed as a single operation remotely, reducing the amount of work done on the overloaded datastore. Other techniques like *leases* ([#sec-leases]) let client-side caches avoid costly invalidation messages.

3. If clients are allowed to interact with weakly-synchronized replicas, the load on the contended shard can be reduced. However, this comes at a significant programmability cost: the illusion of a single copy of data is broken and programmers must now reason about replicated state. Weakly synchronized replicas share updates asynchronously, and clients may communicate with multiple replicas, so they can observe effects out of order, or perform updates which conflict and result in inconsistent states.

To be clear, techniques in the first and second categories can use replicas for fault tolerance and still maintain strong consistency. Strong consistency requires writes to be linearizable [@Herlihy:90:Linear], which can be accomplished with replicas either by funneling through a single master or by a consensus protocol like Paxos [@Lamport:01:Paxos] that makes replicas appear like a single machine. Techniques like E-Paxos [@Moraru:13:EPaxos] would fall under (1) because they expose concurrency while maintaining the single-machine view.

The third category requires much more drastic changes to programming models than the first two because it forces programmers to sacrifice consistency guarantees. However, weak consistency unlocks further improvements to performance properties like availability and lower latency that are impossible with strong consistency. In this work I will focus mostly on techniques that fall into this category because these are the most challenging to understand and require the most significant changes for programmers.

## Balancing requirements {#sec-requirements}
Mitigating contention is just one of the many competing requirements placed on distributed applications. They are expected to be *scalable*, *fault tolerant*, and *highly available*.
Users demand responsive applications, no matter how many other users are active or where they are. The common wisdom is that companies lose money for every increase in response latency. Many systems have service-level agreements (SLAs) promising responses within a certain latency for all but the 99th percentile of requests. Throughput during peak times must be able to keep up with the load.

These performance constraints compete with the desire for *programmability* and *correctness*. Fundamentally, strong consistency cannot be provided with high availability — replication must be exposed in some way.
Strict serializability and ACID transactions are among the many useful programming abstractions that must be broken to achieve those performance properties.
Applications are still expected to appear mostly consistent, so developers are forced to think carefully about how to build correct systems with weaker guarantees and choose where to focus their effort.

Luckily, not everything requires the same level of precision or consistency. 
Some actions, such as selling the last ticket to Star Wars' opening night, require precise, consistent execution. Other situations, such as viewing the number of retweets for a popular tweet, do not need to be exactly correct — users may be satisfied as long as the number is the right order of magnitude.
<!-- Even within the ticket sales example, there is room for relaxation: while there are still thousands of tickets remaining, the exact count shown to users need not be precise, though it ought to be close enough that users can estimate how soon it may sell out.  -->
These *hard* and *soft constraints* can be difficult or impossible to express in current systems. Furthermore, whatever tradeoffs are made to improve programmability or enforce constraints must be balanced against meeting the performance targets, though it is not easy to quantify how changes will affect them.
Programmers of these distributed applications must constantly juggle competing correctness and performance constraints.

## Overview

To understand how the various techniques for managing consistency and performance relate to one another, we will explore them in terms of the properties they trade off:

- *Ordering* and *visibility* constraints between operations
- *Uncertainty* about the state in terms of staleness and possible values

We will also discuss how each technique operates, in terms of:

- *Granularity:* Does it affect the whole system, specific records, or specific operations?
- *Knowledge vs control:* Are users granted additional information about performance or consistency or are they given explicit control?

After exploring the space of existing techniques, we will propose a programming model that incorporates these disparate solutions into a single abstraction – using *abstract data types* (ADTs) to concisely describe application semantics and hide the details of the underlying consistency and coordination techniques. Our implementations, in a distributed data analytics system, *Grappa*, and a prototype ADT-store, *Claret*, show significant performance improvements, especially for high contention workloads. In future work, we propose using *inconsistent, probabilistic, and approximate (IPA) types* to trade off precision in order to take advantage of weak consistency and replication for high availability and scalability.
