# Trading off consistency for performance
In order to meet scalability, availability, and latency requirements, distributed systems programmers must routinely make trade-offs between consistency and performance [@Bailis:HAT;@Brewer:CAP;@Gilbert:CAP]. 
In the *typical* case, consistency does not pose a problem, even for geo-replicated systems – most requests are reads, requests for the same user typically go to the same server, and inconsistency windows after updates are usually small [@Lu:15:Facebook;@Bailis:HAT]. Consistency becomes problematic in those exceptional *high contention* cases and the *long tail* of disproportionately slow requests [@Dean:13:Tail]. However, these cases are almost pathologically bad – high contention not only means many updates that cause inconsistent reads, but also more users makes it more likely to get noticed. The events most likely to cause problems are also the most news-worthy.

Targeting performance for the average case can lead to catastrophic failures in the contentious cases, but the complexity of handling all of the worst cases damages programmability. This burden will become abundantly clear over the next few sections as we look at the axes which programmers must traverse in making their choices.

# Ordering and visibility constraints
Understanding the behavior of a sequence of operations in a program requires knowing all of the ordering and visibility constraints that govern what each operation returns and when each update actually runs. How these constraints are set is determined by the programming model.

## Consistency models
Analogous to memory models in the field of computer architecture, consistency models refer to the allowable reorderings of operations and their visibility in a distributed system. Consistency models differ from memory models primarily in one way: exposing the existence of replication. Due to the reliability and speed of CPU cache hierarchies, memory models can afford to assume strong *coherence* will ensure that there appears to be only one copy of memory. In distributed systems where failure is a possibility and synchronization costs significantly more, it is often necessary to expose replication through the consistency model. This makes them significantly more difficult to reason about – as if memory models were not complex enough as it is.

The strongest consistency model, *strict serializability* (roughly defined as Lamport's *sequential consistency* [@Lamport:79:SC] combined with Herlihy's *linearizability* [@Herlihy:90:Linear]) guarantees that operations appear to occur in a global serial order that all observers agree on and that corresponds to real time. This, and any form of consistency that requires enforcing a *global total order* is theoretically impossible to enforce with *high availability* due to the possibility of network partitions (this is the essence of the CAP theorem [@Brewer:CAP;@Gilbert:CAP]). In practice, strict serializability may not be wholly impractical for the average case, but ensuring it in *all* cases is prohibitively expensive.

At the other extreme, *eventual consistency*, the least common denominator among consistency models, simply guarantees that if update operations stop occurring, all replicas will eventually reflect the same state [@Vogels:EC]. Under this model, programmers cannot count on subsequent operations reflecting the same state, because those operations could go to any replica at any time, and those replicas are continuously receiving updates from other nodes.

There are a whole family of models similar to eventual consistency which add various ordering constraints:

- *Monotonic writes* (MW) ensure that writes from a client are serialized, enforcing *ordering* between writes.
- *Monotonic reads* (MR) ensures that reads will not observe earlier values than have been seen by a particular client already, strengthening *visibility*.
- *Read-your-writes* (RYW) ensures that a client will at least observe its own effects – primarily strengthening an aspect of *visibility*
- *Causal consistency* ensures that operations from different clients causally following a write will observe that write (by some definition of *causation* which the system must track). This means that operations will be *visible to* and *ordered with* each other when applicable.

There are too many variations on these and other models to enumerate, including combinations of them. Each restricts the possible reorderings, making some cases easier for programmers to reason about, while also reducing the flexibility of the system and reducing performance. For instance, many require *sticky sessions* [@Terry:94:Session], which forces clients to continue communicating with a particular replica, even if it is not the fastest, or lowest latency, or most up-to-date one available.

As a way of trading off consistency for performance, weak consistency models are a poor choice. A particular consistency model must often be chosen at a very coarse grain, possibly at the level of an entire database's configuration. Applications can enforce stronger guarantees on top of a weaker model with various consensus protocols, but these must be chosen and implemented carefully, and the extra code written to handle this is not easily adapted to changes in the underlying consistency model.
In general, weak consistency models are not modular: adding or changing an operation in one place may require changing assumptions about ordering elsewhere. Visibility constraints require considering all pairs of interacting operations to determine which orders are allowable.


## Transactions
Transactions are a well-established way to provide stronger guarantees among some operations. By choosing the type and granularity of transactions, programmers have some control over the *ordering* and *visibility* of their operations. 
Like strict serializability, full ACID transactions require a global order so are prohibitive to scaling and high availability, leading to the development of weaker transaction models.
As the antithesis of the strong guarantees of ACID, some have termed these weaker semantics *"BASE"* (Basically Available, Soft state, Eventually Consistent) [@Pritchett:08:BASE]. Luckily, programmers are not restricted to simply choosing between these two extremes; some have proposed ways to bridge the gap.

### Transaction chopping and chaining
Some techniques can expose limited extra concurrency between transactions without requiring programmers to sacrifice ACID semantics. Transaction chopping [@Shasha:95] automates a task programmers could do by hand: breaking transactions into minimal-sized atomic pieces, determined by a static analysis of interleavings. A more recent system, Lynx [@Zhang:13:Lynx] executes split transactions as a *chain*, hopping from shard to shard, coordinating the order of execution to allow conflicting transactions to safely interleave. Finally, Callas [@Xie:15:ACIDAlt] groups transactions that commute with each other to allow them to execute concurrently. Transaction boosting [@Herlihy:PPoPP08] similarly allows transactions to overlap when they commute, but at the level of individual operations.

### Salt: Combining ACID and BASE
Just as a small fraction of data items are responsible for the majority of contention, the same is true for transactions. Rather than forcing programmers to give up ACID semantics for their entire application, Salt [@Xie:14:Salt] allows transactions with BASE semantics to coexist safely with ACID transactions. Using new locking schemes, they ensure that transactions executing with weaker BASE semantics cannot violate the strong safety guarantees of the ACID transactions.

Converting an ACID transaction to execute without those guarantees is an error-prone task; it involves considering all the new possible interleavings and establishing how to resolve all the possible conflicts without coordination. Salt's model means that programmers only need to "BASE-ify" the transactions causing performance or scaling problems. This makes it relatively straightforward to trade off consistency where necessary, but does not do much to help programmers deal with the weaker semantics.

### RAMP transactions
Even without guarantees of a serializable total order, there are still benefits to supporting atomic updates: preventing foreign key constraint violations, and ensuring that indexes and other derived data are as up-to-date as the backing data. In support of these safety properties, *RAMP (Read Atomic Multi-Partition) transactions* provide coordination-free atomic visibility for multiple updates [@Bailis:14:RAMP]. They work by *staging* updates on all participating shards so they can force the complete set of updates for a transaction to be made *visible* at one point in time. They dynamically detect racy reads and fix them by either selecting an appropriate staged version or waiting for another round of communication. Because these determinations are made locally, they cannot guarantee global mutual exclusion or serializability.

## Models inspired by distributed version control {#sec-revisions}
One of the troubles with weakly consistent replication is that it is often not possible to construct a serializable history of an execution, which makes it very difficult to reason about which effects were visible to different clients at various points in time. Distributed version control systems (DVCS) like Git have inspired alternative ways of viewing concurrent execution. DVCSs allow individuals to work concurrently on *forks* or *branches* without interference and resolve conflicts at explicit *merge* points later. These histories are not serializable, but they still allow users to easily understand when effects become visible to different observers.

The *Push/Pull Model of Transactions* [@Koskinen:15:Pushpull] formalized several consistency and transaction models in these terms inspired by DVCS. A model called *branch cnsistency* proposed for a system called TaRDIS [@Crooks:15:Tardis] uses the notion of branching and merging for isolation and conflict resolution in geo-replicated systems, delegating conflict resolution to applications.

*Concurrent revisions* [@Burckhardt:10:Revisions] proposes an execution model built around forking and joining state along with concurrent execution to make sharing explicit.
In this model, concurrent tasks *fork* a copy of the state they access. Changes to forked state are only visible to that task and its descendants until the concurrent task is *joined* back in. On join, changes to forked data items are *merged* according to their type. For example, as a cumulative type, a forked `Counter` tracks increments made to it, and when *joined*, adds those increments to the original value. In this way, multiple concurrent tasks can increment the shared `Counter` without conflicting and it is clear exactly when their effects are made visible. Follow-on work extended the ideas of concurrent revisions and revision diagrams to reason about eventual consistency: in eventually consistent transactions [@Burckhardt:12:ECTxn] and  mobile/cloud applications [@Burckhardt:12:Cloud].

These programming models show that there is hope for reasoning about weakly consistent replicated data. The *isolation types* and *cloud types* from concurrent revisions provide useful semantics for working with highly contended data. In these models, trading off consistency for performance is not as clear-cut; *revision diagrams* and DVCS histories imply strict coordination points. It is also unclear how to enforce global constraints on forked data. Consider again the ticket sales example: in order to ensure tickets are not over-sold, concurrent forks must somehow know how many of the remaining tickets they are allowed to sell, without knowing how many other forks exist, breaking the abstraction.

## Annotating constraints
~ Fig {#fig-annotations caption="*Annotating application invariants.* Annotations are used to determine where coordination is necessary and what consistency is required to enforce it." }
![annotations](fig/annotations.pdf)
~

Several datastores allow consistency levels to be specified on a per-operation basis: research systems Gemini [@Li:12:RedBlue] (RedBlue consistency), and Walter [@Sovran:11:Walter], and production systems Cassandra [@cassandra] and Riak [@riak] (per object or namespace). However, they leave programmers to determine where to use stronger consistency in order to achieve their correctness goals, a very error-prone task. Recent work has explored ways of *automatically* choosing the correct consistency level or coordination strategy based on manual *annotations*.

Sieve [@Li:14:Sieve] builds on top of Gemini, automatically determining how to implement the desired semantics with causal consistency and adding additional synchronization wherever strong consistency is needed. This relies on programmer-specified global invariants and annotations on the relational database schema to select the desired merge semantics in case of conflicts. These annotations echo the variants of CRDTs (see [#sec-crdts]).

Quelea [@Sivaramakrishnan:15:Quelea] has programmers write *contracts* to describe ordering constraints between operations and then automatically selects the correct consistency level for each operation to satisfy all of the contracts. Contracts are specified in terms of low-level consistency properties such as *visibility* and *session order*.
For example, to ensure a non-negative bank account balance, a contract indicates that all `withdraw` operations must be visible to one another, forcing the operation to be executed with sequential consistency.
Because correctness properties are specified *independent of a particular consistency model*, or set of consistency levels, they are *composable* with each other and *portable* to other datastores supporting different consistency options. However, the low-level primitives used in contracts may not be intuitive for programmers and still require reasoning about all the possible anomalies between operations.

Indigo [@Balegas:15:Indigo] takes a different approach to expressing application requirements: instead of specifying visibility and ordering constraints, programmers write *invariants over abstract state and state transitions*, and annotate *post-conditions* on actions to express their side-effects in terms of the abstract state. They then perform a static analysis to determine where concurrent execution could violate the invariants and add coordination logic to avoid those conflicts. 

[#fig-annotations] shows how annotations would be written in these three systems to implement our running ticket sales example. In this case, the desired invariant is that tickets are not over-sold – that is, the count of remaining tickets should be non-negative. Sieve and Indigo can enforce this invariant directly as written. Quelea's visibility-based contracts cannot tightly describe this invariant; instead, they must be conservative and force ticket sales to be strongly consistent.

Indigo's approach provides an excellent way to express application-level semantics and have the system automatically figure out how to enforce them. The primary downside is that abstract state must be modeled separately from the true application state. Additionally, though their invariants can specify hard constraints, they do not have a way to express the soft constraints we discussed in [#sec-requirements].

## Review: Constraints
~ Fig {#fig-constraints .zoom max-height=20em caption="Ordering and visibility constraints." }
~~ Center
![constraints](fig/constraints.pdf){max-height=17em}
~~
~


[#fig-constraints] organizes the systems we have so far discussed along axes of increased ordering and visibility constraints. There is no *best* point in this space – increased ordering constraints come with restrictions which limit availability and increased latency. The best solutions, therefore, provide the most flexibility and control over the constraints they can enforce.

Weak consistency models provide little in the way of ordering or visibility guarantees. With eventual consistency at the very bottom, Read-Your-Writes additionally constrains operations within a session to be visible to one another, while Monotonic Writes force some order among writes, and Monotonic Reads the *visibility* of those ordered writes.
Of the consistency models, Causal provides the most programmer control – essentially arbitrary constraints can be created by forcing the system to consider them causally linked.

RAMP transactions and DVCS-like techniques allow explicit control over visibility but little control over ordering between transactions (or branches/forks), placing them at the top left of [#fig-constraints].
Because Salt allows weaker transactions to interoperate with ACID transactions, it allows programmers to select from range of ordering and visibility levels depending on need.
Cassandra, Riak, Gemini, and Walter allow per-operation consistency levels to be set, which also provides a range of constraints, but with significant complexity for programmers.

The annotation-based techniques provide the most control over these constraints and also automate part of the task of choosing them. Quelea, with contracts specifically on these constraints, provides the most control, but at a lower level of abstraction than Indigo or Sieve's application-level invariants.
So far, we have been dealing mostly in terms of plain reads and writes; the next section will show how to do better by placing bounds on the *allowable values*  and *staleness*.

# Uncertainty
Factoring out the knowledge provided by ordering and visibility constraints, the state observed by operations is *uncertain* – subject to constraints on what *values* the piece of data may hold and how out-of-date the replica is.
<!-- In the presence of any weak replication protocol, it takes some amount of time for updates to propagate, but so far none of the models discussed have been concerned about that staleness property. -->

## Restricted values
One of the problems with programming with eventual consistency is ending up with conflicting writes that overwrite one another in unpredictable ways. The solution to this without enforcing mutual exclusion somehow is to define commutative deterministic *merge* functions that resolve conflicts resulting from concurrent updates. The vision of Bayou [@Terry:95:Bayou] was that applications would define custom merge functions over all of the application state so that users could work offline and automatically synchronize their changes next time they connected again. Though this has not caught on in any major way due to the difficulty (and non-modularity) of handling all possible combinations of updates in a way that is satisfactory to users, it has led to the development of libraries of data structures with this property.

### CRDTs {#sec-crdts}
*Convergent or conflict-free replicated data types* (CRDTs) are data types that have such deterministic merge functions defined for them. Resolving conflicts deterministically requires making choices about the semantics of concurrent updates, leading to a proliferation of CRDTs for various use cases. Even simple data structures like Sets must have multiple variants such as those in [#tab-set-crdts] that resolve non-commuting operations (for sequential data types) differently.

~ Tab {#tab-set-crdts .zoom caption="Example `Set` CRDTs. Variations are due to the fact that `add` and `remove` do not commute for a sequential `Set`."}
~~ HtmlOnly
|:-{width=0.6in}-|:---------------------------|
| **G-Set**     | "Grow-only" set, `remove` is simply disallowed |
| **PN-Set**    | A counter per item matches `add`'s and `remove`'s, the set contains the item whenever there are more `add`'s. |
| **OR-Set**    | "Observed-remove" set where causally-related `remove`s are observed, but `add` wins over `remove` when concurrent. |
|---------------|-------------------------------|
~~
~~ TexRaw
\begin{tabular}{p{0.1\linewidth}p{0.82\linewidth}}
\hline
\textbf{G-Set}     &``Grow-only'' set, \mdcode{remove} is simply disallowed \\
\textbf{PN-Set}    & A counter per item matches \mdcode{add}'s and \mdcode{remove}'s, the set contains the item whenever there are more \mdcode{add}'s. \\
\textbf{OR-Set}    & ``Observed-remove'' set where causally-related \mdcode{remove}s are observed, but \mdcode{add} wins over \mdcode{remove} when concurrent. \\
\hline
\end{tabular}
~~
~

CRDTs can be enormously useful because they allow concurrent updates to accumulate changes. Riak [@riak] implements several data types and encourages their use. Like ADTs,  CRDTs also provide a well-defined set of possible values that a variable can hold – restricted to changes made by supported operations. This is a clear advantage over simple *registers* that are completely overwritten on each write, making them unpredictable when the order of updates is uncertain. CRDTs can still suffer from many of the effects of eventual consistency – updates applied to different replicas mean clients could see divergent changes for some time – convergence does nothing to change that. Keep in mind that divergence *could* continue indefinitely thanks to eventual consistency – actual times are the subject of the next form of uncertainty we will discuss.

### Bloom
The philosophy of Bloom [@Alvaro:11:Bloom;@Conway:12:BloomL] is to find ways to write programs that avoid the need for coordination as much as possible. The *CALM principle* (Consistency And Logical Monotonicity) advocated by this work formalizes the requirements for an entire program to be eventually consistent, obviating any need for coordination. It is built around the notion of monotonicity, where programs compute sets of facts that grow over time so that information is never lost and convergence can be guaranteed.

In the Bloom model, programmers express applications as *statements* about monotonically growing sets of *facts*. These facts can be encoded as sets or other collections with suitable *merge functions* ensuring values of the type have a well-defined partial order, such as CRDTs [@Conway:12:BloomL]. Bloom statically ensures that programs compose these types in ways that are monotonic.

In its pure form, Bloom's programming model requires substantial changes to most applications, so later work on Blazes [@Alvaro:14:Blazes] showed how the monotonicity analysis could be applied to find where coordination is necessary in existing distributed applications by annotating components with Bloom's properties. This style of programming is somewhat different than the other annotation-based approaches of Indigo and Quelea because it focuses on *sealing* streams on coordination points. Bloom and its variants can ensure *eventual* consistency, but this says nothing about how long it will take or what intermediate states will be observable, so in practice, users would still have to worry about observing stale or inconsistent states.

### Escrow and Reservations {#sec-escrow}
*Escrow* is a term from banking and legal proceedings where some amount of money is set aside and held by a third party in order to ensure it will be available for use at a later time after some (typically legal) condition is met. In database systems, this term has been borrowed for use in concurrency control to refer to "setting aside" some part of a record to be later committed.

O'Neil's idea of *escrow* [@ONeil:86] came from work on Fast Path [@Gawlick:85] and Reuter's Transactional Method [@Reuter:82]. The idea was to increase concurrency on *aggregate fields*, such as fields keeping track of a count or a sum, which could become hot spots because they were updated frequently. The idea of *escrow* is to split up an aggregate value into a *pool* of partial values and allocate parts from the pool to transactions when they execute so that when they are ready to commit, they are guaranteed to be able to. For example, if a transaction is going to decrement an account balance as long there are sufficient funds, it will hold the amount it wishes to decrement *in escrow*. Other transactions can also decrement the balance, provided combined they will not leave the balance negative. If a transaction aborts, the escrowed values are returned to the pool.

Escrow can be extended to any *fragmentable object* [@Walborn:95], that is, any data type providing a way to *split* itself into fragments of the same type, and a way to later *merge* the fragments back together. This is essentially the same criteria as for *monoids* used for aggregators in Summingbird [@summingbird], although used in the opposite way, and similar to the isolation types in Concurrent Revisions [#sec-revisions].

*Reservations* [@Preguica:03] can be thought of as escrow for replicated data. A reservation pre-allocates permission to do updates so that in the future they can be done without coordination. This moves coordination off the critical path and allows synchronization to be amortized when multiple updates share a reservation. For example, in Mobisnap [@Preguica:03] where they were first introduced, a salesman might reserve a number of tickets or a some quantity of a commodity, then while on the go, without connectivity, make a sale and know that it is safe to do so. Combining reservations with *leases* (discussed earlier in [#sec-leases]) allows them to be reclaimed automatically after a certain time has elapsed. Exo-leasing [@Shrira:08:ExoLeasing], not quite using *lease* the same way, further extends escrow and reservations to be decentralized and exchangeable among offline clients.

Reservations are easy to generalize — Indigo [@Balegas:15:Indigo] uses them to implement its application-specific conflict avoidance logic that includes auto-generated code. A similar implementation of numeric invariant preservation called *bounded counters* [@Balegas:15:Invariants] was built on top of Riak, an eventually consistent datastore.

One problem that arises in distributing reservations is imbalance: if some replicas receive more requests than others, they may use up all of their reserved updates before others do. In these situations, techniques such as the *demarcation protocol* [@BarbaraMilla:94:Demarcation;@Balegas:15:TowardsInvariants], or *handoff* [@Baquero:13:Handoff] can allow replicas to redistribute permissions or re-balance per-replica limits. These techniques operate similar to *work-stealing* in Cilk [@Agrawal:07:Workstealing].


## Bounded staleness
In theory, eventually consistent systems provide absolutely no guarantees during execution because there is no bound on the time it must take for updates to propagate, and there are almost no situations where updates are guaranteed not to occur. In practice, however, programmers typically observe very few actual consistency errors, even at large scale [@Lu:15:Facebook]. This is because the propagation time, or *inconsistency window* is typically very small, on the order of tens of milliseconds [@Bailis:12:PBS], so few accesses observe the gap. However, programmers cannot rely on these observations because they do not hold in all cases. High contention situations are particularly problematic because with more concurrent updates and accesses, the chances of observing inconsistencies is much higher, and the value is also likely to be further from the correct value.

### Leases {#sec-leases}
The problem with reads is that by the time the client gets the result, it could already be out of date, even without the additional complexity introduced by eventual consistency. Leases are a way of communicating how old a read is, by associating it with a time in the future when it should be considered stale.
First proposed for file system caches to avoid needing to send explicit invalidations [@Gray:89], they are now used in application caches in modern datacenters, such as in Facebook's Memcache system [@Nishtala:13:Memcache]. In addition to simply bounding staleness without explicit invalidation, leases can be used to indicate a promise that the value will not be updated for some time.

*Warranties* [@Liu:14:Warranties] extend leases to grant permission for holders to perform specific updates for a fixed amount of time. This has the benefit that permissions are automatically reclaimed after the time has elapsed, even if the holder crashed, and saving a reply message if the recipient does not wish to use the warranty. 

### Probabilistically bounded staleness (PBS)
In order to help programmers reason about staleness, Bailis et al. [@Bailis:12:PBS] introduced a metric called *probabilistically bounded staleness* (PBS) which quantifies the staleness of accesses, either in terms of *time* or *versions*. By observing the distributions of propagation delays, round trip times, and rate of updates, their implementation builds a model of the system and uses it to predict staleness during execution. Their observations on a production system confirmed that 
They also discussed how, by choosing the number of replicas to send writes to or consult for reads, one can control staleness and suggested how PBS could be used to select the right balance.

### Consistency-based SLAs
~ Center
~~ Fig { #fig-pileus .zoom caption="Example Consistency-based SLA: Shopping cart." }
![SLA code example](fig/sla.pdf){max-height=7.5em}
~~
~

With *consistency based SLAs* [@Terry:13:SLAs], programmers can explicitly trade off consistency for latency. A consistency SLA specifies a target latency and a consistency level (e.g. 100 ms with read-my-writes). In this programming model, operations specify a set of desired SLAs, each associated with a *utility*. Using a prediction mechanism similar to PBS, the Pileus system attempts to determine which SLA to target to maximize utility, typically to achieve the best consistency possible within a certain latency.

Allowing users to specify their desired latencies and consistencies directly to the system is powerful. However, because it is so fine-grained, the burden of choosing target latencies and consistency for each operation could be quite high, and it seems difficult to compose a sequence of operations and SLAs to achieve an overall target latency or correctness criteria.

## Review: Uncertainty
~ Fig {#fig-uncertainty .zoom max-height=24em text-align=left caption="Bounding *uncertainty*  in terms of staleness and restricting values." }
~~ Center
![uncertainty](fig/uncertainty.pdf){max-height=21em}
~~
~

[#fig-uncertainty] maps out the techniques we have covered along two axes: how much they bound staleness versus how they restrict the set of allowable values. Again, eventual consistency provides the least guarantees. In fact, most of the techniques covered earlier do not affect *staleness* hardly at all. However, stronger consistency models and the annotation-based techniques do restrict possible values by eliminating some conflicts. The major differentiator comes with the switch to using CRDTs to restrict operations to those supported for each data type, like with ADTs. Bloom, by restricting to monotonic transformations over Sets or CRDTs, has the most bounded values.

Escrow and reservations can be very useful for bounding the uncertainty of replicated data. They allow hard bounds to be enforced without preventing parallelism in most cases. Consider again the Star Wars ticket sales example from [#sec-intro]. In order to handle the high load, we could replicate the tickets for this movie and allow clients to purchase tickets from any replica, but then we would not be able to prevent the same ticket from being sold to two different users. Using the concept of *escrow*, however, we can divide the tickets among the replicas, allowing clients to purchase them in parallel while there are many remaining, yet preventing tickets from over-selling when they begin to run out. We will discuss this more in [#sec-disciplined].

Along the axis of *staleness*, PBS provides additional knowledge, but little control. Leases, on the other hand, allow for a range of information about staleness to be conveyed, and warranties provide additional control over how values change by controlling permission to perform updates. Consistency SLAs allow latencies and consistency levels to be traded off, giving control over both axes of uncertainty to applications.
