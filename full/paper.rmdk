title:       Claret
subtitle:    Abstract Data Types in Data Stores

author:      Brandon Holt, Irene Zhang, Dan Ports, Mark Oskin, Luis Ceze
affiliation: University of Washington
email:       {bholt,iyzhang,dkrp,oskin,luisceze}\@cs.uw.edu

Heading Base: 2
Bib style:    plainnat
Bibliography: refs.bib
doc class:    [preprint,nocopyrightspace]style/sigplanconf.cls
css:          style/style.css
Logo: False

~Pre, ~Code: font-family="Inconsolata" font-size=10pt

~Fig: label='Figure [@fig]{.figure-label}' replace='&source;&nl;[[**&label;.** ]{.caption-before}&caption;]{.figure-caption html-elem=fig-caption}&nl;{notag}&nl;'

~ TexRaw
\cssClassRuleDo{fig}{%
  \cssDoBefore{\begin{figure}[t]}%
     \cssDoAfter{\end{figure}}%
}
~

```{r setup, include=FALSE}
opts_chunk$set(dev='pdf', echo=F, message=F, warning=F, error=F, fig.width=3.6, fig.height=3)
```

[TITLE]

~ Abstract
Out of the many NoSQL databases in use today, some that provide simple data structures for records, such as Redis and MongoDB, are now becoming popular. Building applications out of these complex data types provides a way to communicate intent to the database system without sacrificing flexibility or committing to a fixed schema. Currently this capability is leveraged in limited ways, such as to ensure related values are co-located, or for atomic updates. There are many ways data types can be used to make databases more efficient that are not yet being exploited.

We explore several ways of leveraging abstract data type (ADT) semantics in databases, focusing primarily on commutativity. Using a Twitter clone as a case study, we show that using commutativity can reduce transaction abort rates for high-contention, update-heavy workloads that arise in real social networks. We conclude that ADTs are a good abstraction for database records, providing a safe and expressive programming model with ample opportunities for optimization, making databases more safe and scalable.
~

# Introduction

The move to non-relational (NoSQL) databases was motivated by a desire for scalability and flexibility. People found that by giving up strong consistency, they could better scale services to millions or billions of users while meeting tight performance goals.
Because of inherent uncertainty in timing and connectivity, in many cases users are likely to accept minor inconsistencies such as two tweets being out of temporal order or needing to retry an action. In such cases, relaxed consistency feels like a natural solution, but it leaves much to chance: there is likely no guarantee that more significant inconsistencies are impossible.
When consistency is critical, developers can enforce stronger guarantees manually, or use serializable transactions in systems like Google's Spanner [@Spanner], but this leaves them with two extremes with a significant performance gap.
If certain parts of an application can tolerate imprecision, why not capture those properties in the programming model? Is there a way programmers can express the semantics they desire succinctly and precisely, helping the database optimize performance and scalability, without sacrificing flexibility?

We propose abstract data types (ADTs) as the solution. Rather than limiting the records in databases to primitive types like strings or integers, raising them to more complex data types provides a richer interface, exposing ample opportunities for optimization to the database and a precise mechanism to express the intentions of programmers. In this work we explore several ways of leveraging commutativity and data types to improve database performance and allow programmers to make tradeoffs between performance and precision, starting by demonstrating one way of using commutativity to reduce transaction aborts.

# Commutativity {#comm}

Commutativity is well known, especially in distributed systems, for enabling important optimizations. Since the 80s, commutativity has been exploited by database systems designers [@Weihl:1988;@Fekete:90] within the safe confines of relational models, where complete control of the data structures allows systems to determine when transactions may conflict. Recently, commutativity has seen a resurgence in systems without a predefined data model, such as NoSQL databases and transactional memory.
Eventually consistent databases use commutativity for convergence in work such as RedBlue consistency [@Li:OSDI12] and conflict-free replicated data types (CRDTs) [@Shapiro:SSS11]. Other systems specialize for commutative operations to improve transaction processing, such as Lynx [@Zhang:SOSP13] for tracking serializability, Doppel [@Narula:OSDI14] for executing operations in parallel on highly contended records, and HyFlow [@Kim:EuroPar13] for reordering operations in the context of distributed transactional memory. We propose unifying and generalizing these under the abstraction afforded by ADTs.

~ Fig { caption="Code snippet." }
```ruby
def hello(name)
  puts "Hello, #{name}!"
end
```
~

# Evaluation {#eval}

To demonstrate the efficacy of leveraging commutative operations in transactions, we built a simple prototype key-value store, modeled after Redis, that supports complex data types for records, each with their own set of operations. Our experiments were carried out with 4 shards on 4 local nodes, each with 8-core 2GHz Xeon E5335 processors and standard ethernet connecting them.

## Transaction protocol

The transaction protocol employs standard two-phase commit and two-phase locking with retries to ensure isolation, atomicity. We implement a number of standard optimizations, such as delaying acquiring locks for operations that don't return a value to the *prepare* step so that locks are held for as short a time as possible. However, there is one step that is non-standard in order to support complex data types where rolling back state changes would be non-trivial.

To support transactions with arbitrary data structure operations, each operation is split into two steps: *stage* and *apply*. During transaction execution, each operation's *stage* method attempts to acquire the necessary lock and may return a value *as if the operation has completed* (e.g. an "increment" speculatively returns the incremented value). When the transaction is prepared to commit, *apply* is called on each staged operation to actually mutate the underlying data structure. This allows operations to easily be un-staged if the transaction fails to acquire all the necessary locks, without requiring rollbacks.

Commutativity comes into play in the locking scheme. Using the algorithms from [@Kulkarni:PLDI11] and our commutativity specifications, we design an abstract lock for each record type. Our `SortedSet`, for instance, has an `add` mode which allows all insertion operations to commute, but disallows operations like `contains` or `size`.
As a baseline, we implement a standard reader/writer locking scheme that allows all read-only operations to execute concurrently, but enforces that only one transaction may modify a record at a time.

~ Fig { #stress-tput caption="Throughput of raw Set operations." }
```{r stress, include=T, fig.width=3.1, fig.height=3.0}
d <- data.papoc(where="name like 'stress-v0.14%'")
d$opmix <- factor(revalue(d$mix, c(
    'mostly_update'='35% read\n65% update',
    'update_heavy'='50% read\n50% update',
    'read_heavy'='90% read\n10% update'
)))
d$dist <- factor(revalue(d$alpha, c('0.6'='Zipf: 0.6', '-1'='Uniform')))
d.u <- subset(d, nshards == 4 & nkeys == 10000 
    & (alpha == '0.6' | alpha == '-1') 
    & grepl('update_heavy|read_heavy', mix)
)
ggplot(d.u, aes(x=nclients, y=throughput/1000,
        group=cc, fill=cc, color=cc, linetype=cc))+
    stat_summary(fun.y=max, geom="line", size=0.4)+
    xlab('Concurrent clients')+ylab('Throughput (k/sec)')+
    expand_limits(y=0)+
    facet_grid(dist~opmix)+
    theme_mine+
    theme(legend.position=c(0.5,-0.25), plot.margin=unit(c(.5,.5,10,.5),'mm'), legend.direction='horizontal', legend.title.align=1)+
    cc_scales(title='Concurrency\ncontrol:')
```
~

We first evaluate performance with a simple workload consisting of a raw mix of `Set` operations randomly distributed over 10,000 keys. We use both a uniform random distribution as well as a skewed Zipfian distribution with a coefficient of 0.6. In [#stress-tput], we see that commutative transactions perform strictly better, showing the most pronounced benefit over the more update-heavy, skewed workload.

## Case study: Retwis
To understand performance on a typical web workload, we use *Retwis*, a simplified Twitter clone designed originally for Redis [@redis]. Data structures such as sets are used track each user's followers and posts and keep a materialized up-to-date timeline for each user (represented as a sorted set). On top of Retwis's basic functionality, we added a "repost" action that behaves like Twitter's "retweet".

# References {-}
[BIB]
