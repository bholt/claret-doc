# Expressing abstract behavior

Just as in any software design, building Claret applications involves choosing the right data structures. There are many valid ways of composing ADTs within transactions for a correct implementation, but to achieve the best performance, the programmer must express as much of the high-level abstract behavior as possible through ADT operations, such as atomicity and commutativity.

Typically, the more specialized an ADT is, the more concurrency it can expose, so finding the closest match for each use case is essential.
For example, an application needing to generate unique identifiers should not use a counter, which must return the next number in the sequence, because this is very difficult to scale (as implementers of TPC-C [@TPCC], which explicitly requires this, know well). Instead, a `UniqueID` type succinctly expresses that non-sequential IDs are okay, and can implement it in a way that is fully commutative.

Claret has a library of pre-defined ADTs, shown in [#fig-adt-list] which were used to implement the applications in this work.
Reusing existing ADTs saves implementation time and effort, but may not always expose the maximum amount of concurrency.
Custom ADTs can express more complex application-specific properties, but the developer is responsible for specifying the abstract behavior for Claret.
[todo: example: bank account adt? (not one of our apps, but there is an easy-to-describe custom adt there)]
The next sections will show how ADT behavior is specified in Claret to expose abstract properties of ADTs for use in the optimizations previously described.

~ Fig { #fig-adt-list caption="Library of built-in data types."}
| Data type      | Description |
|:---------------|:-{width=2.32in}-|
| `UIdGenerator` | Create unique identifiers (not necessarily |
|                | sequential) (`next`) |
| `Dict`         | Map (or "hash") which allows setting or |
|                | getting multiple field/value pairs atomically |
| `ScoredSet`    | Set with unique items ranked by an |
|                | associated score (`add`, `size`, `rangeByScore`, ...) |
| `TopK`         | Like `ScoredSet` but keeps highest ranked 
|                |items (`add`,`max`) |
| `SummaryBag`   | Tracks summary stats like mean/max of |
|                | "added" items (`add`, `mean`, `max`)|
|--------------|------------|
~

## Commutativity Specification {#sec-commutativity-spec}

~ Fig { #fig-spec caption="Abstract Commutativity Specification for Set." }
| method:            | commutes with: | when:                       |
|:-------------------|:---------------|:----------------------------|
|`add(x): void`      | `add(y)`       | $\forall x, y$              |
|`remove(x): void`   | `remove(y)`    | $\forall x, y$              |
|                    | `add(y)`       | $x \ne y$                   |
|`size(): int`       | `add(x)`       | $x \in Set$                 |
|                    | `remove(x)`    | $x \notin Set$              |
|`contains(x): bool` | `add(y)`       | $x \ne y \lor y \in Set$    |
|                    | `remove(y)`    | $x \ne y \lor y \notin Set$ |
|                    | `size()`       | $\forall x$                 |
|--------------------|----------------|-----------------------------|
~

*Commutativity* is not a property of an operation in isolation.
A *pair* of operations commute if executing them on their target record in either order will produce the same outcome. Using the definitions from [@Kulkarni:PLDI11], whether or not a pair of method invocations commute is a function of the methods, their arguments, their return values, and the *abstract state* of their target. We call the full set of commutativity rules for an ADT its *commutativity specification.* An example specification for a *Set* is shown in [#fig-spec]. <!-- There are actually many valid specifications which expose less than the maximum commutativity but may be cheaper to implement. -->
However, we need something besides this declarative representation to communicate this specification to Claret's concurrency controller.

### Abstract lock interface

~ Listing { #lock-interface caption="Interface for expressing commutativity for a data type. Typical implementations use *modes* to easily determine sets of allowed operations, and a *set* of lock-holders to keep track of outstanding operations." }
![Abstract lock interface](fig/abstract-lock.pdf){.onecol}
~

In Claret, each data type describes its commutativity by implementing the *abstract lock* interface shown in [#lock-interface]. This imperative interface allows data types data types to be arbitrarily introspective when determining commutativity. In our pessimistic (locking) implementation, the client must acquire locks for its operations before executing them. When the datastore receives a lock request for an operation on a record, the concurrency controller queries the abstract lock associated with the record using its `acquire` method, which checks the new operation against the other operations currently holding the lock to determine if it can execute concurrently (commutes) with all of them.

A traditional reader/writer lock can be implemented in this way by tracking all the transactions currently reading a record when it's in *reading* mode, only allowing a write (*exclusive* mode) when all readers have released their locks. More permissive abstract locks can be implemented simply by adding additional modes, such as *appending* for sets or lists, which allow all *add* operations. More fine-grained commutativity tracking can be done with more specialized implementations. For instance, a set `add` could acquire the lock during a read-only mode if the set already contains the item it adds.

### Transaction boosting

Claret uses abstract locks to perform transaction boosting. With these locks, programmers can be assured that transactions will only conflict with each other on operations that do not commute. In our auction application, `bid`s are performed by adding to a `zset` (as in [#fig-adt-bid]). For popular items and near the end of auctions, the `ItemBids` `zset` will be the most highly contended, but because `add`s commute, those `bid` transactions will not conflict with each other.

## Phaser

~ Listing { #lst-phaser caption="Phaser interface (example implementation): `enqueue` is called after an operation fails to acquire a lock, `signal` is called when a phase finishes (all ops in the phase commit and release the lock)." }
![Phaser interface](fig/phaser.pdf){.onecol}
~

The phasing optimization described in [#sec-phasing] requires knowing how to divide operations for each ADT into phases. Claret accomplishes this by pairing a *phaser* with the abstract lock on each record.
With phasing enabled, whenever an operation fails to acquire a lock, the concurrency controller *enqueues* the operation into the *phaser* for that record. When all operations in a phase have committed, the abstract lock *signals* the phaser, requesting operations to start a new phase.
The simplest phaser implementations simply keep queues corresponding to modes (sets of operation types that always commute with one another). [#lst-phaser], for example, shows `adders`, which will contain all operations that may insert into the list (just `add`), and `readers`, which includes any read-only operations (`size`, `contains`, `rangeByScore`, etc).
More complicated phaser implementations may allow operations to have multiple possible modes or more complicated state-dependent ways of determining which operations to signal when.

## Combiner

~ Fig { #fig-combining caption="Combining `rangeByScore` operations: the second operation's result can be computed locally from the first because its range is a subset of the first, so the two can be combined." }
![Combiner for rangeByScore](fig/combiner.pdf){.onecol}
~

Finally, ADTs wishing to perform combining ([#sec-combining]) must implement a *combiner* to tell Claret how to combine operations.
Combiners only have one method, `combine`, which attempts to match the provided operation against any other outstanding operations (operations that have acquired a lock but not committed yet).

Remember from [#sec-transactions] that operations are split into *prepare* and *commit*. Combining is only concerned with the *prepare* part of the operation. Operations that do not return a value (such as `add`) are simple to combine: any commuting mutating operations essentially just share the acquired lock and commit together. Operations that return a value in the *prepare* step (any read), can only be combined if they can share the result. For the `zset.size`  operation, this is simple enough: all concurrent transactions should read the same size, so combined `size` ops can all return the size retrieved by the first one. [#fig-combining] illustrates a more complex example of combining two `rangeByScore` operations on a `zset`. The two operations can be combined because the second operation requests a sub-range of the first, so the combiner can compute the correct locally.

To avoid excessive matching, only operations declared *combinable* are compared. The client-side library, as discussed earlier, keeps track of outstanding combinable operations with a map of combiners indexed by key. Before sending an acquire request for a combinable operation, the client checks the combiner map for that key. If a combiner is not found, or the combine fails, the operation is sent to the server as usual. If it succeeds, the result of the acquire stage is returned immediately, and Claret handles merging the two transactions as described before.
