# Leveraging data types

At the application level, actions like `PlaceBid` may obviously commute with one another, but once these actions have been broken down into individual `put` and `get` operations, that knowledge is lost. The datastore no longer has any way to see that which reorderings are allowable, so adding multiple items to a `Set`, for instance, just looks like a series of writes with some implicit order. We need some way to communicate these higher-level properties about the application to the storage and execution system.

As was alluded to earlier, data types hold the key. Specifically, the notion of *abstract data types* (ADTs) logically decouples the abstract behavior, where operations may be commutative, associative, monotonic, etc, from the low-level implementation that instantiates that behavior with lower-level reads and writes.

Reasoning at the level of ADTs is good for programmers and datastores alike. Higher-level ADT operations expose more flexibility to the datastore, such as chances for reordering or cheaper ways of executing them. At the same time, programmers enjoy re-using existing data structures and have control over which data types they use to build their applications, including designing their own. ADTs can range from simple primitive data types like `Set`, to fully application-specific types, like an `Auction` data type with commutative `PlaceBid` operations.

General transactions allow ADT operations to be composed to perform more complex actions. The level of complexity of the data types is up to the programmer and can affect performance. Consider three implementations shown in [#placebid]. The first, using generic records, is correct but doesn't expose commutativity. The second and third ones express commutativity, but the third uses a custom `Auction` data type, while the second uses a standard Redis-style `SortedSet` ADT, whose `add` operations naturally commute. In this case, these last two should have roughly the same `PlaceBid` performance, but in other cases a single built-in data type is not sufficient, but may still express significant concurrency.

~ Listing { #placebid caption="Three different pseudocode implementations of `PlaceBid`, showing various levels of ADT complexity. [todo: are these (made-up) bindings intuitive?]" }
```python
# Generic records: not commutative
def transactionPlaceBid_A(item, bidder, price):
  Record('UserBid:'+bidder+item).put(item)
  if price > Record('MaxBid:'+item).get():
    Record('MaxBid:'+item).put(price)
    Record('MaxBidder:'+item).put(bidder)

# Built-in ADTs: expresses commutativity, reusable
def transactionPlaceBid_B(item, bidder, price):
  SortedSet('ItemBid:'+item).add(price, bidder)
  Set('UserBid:'+bidder).add(item)

# Custom ADT: maximum expressivity, app-specific
def transactionPlaceBid_C(item, bidder, price):
  Auction(item).placeBid(price, bidder)
```
~

Considering that we know the abstract semantics of operations on data types, how can we leverage them specifically in datastores? The next few sections will describe some existing techniques which can be applied to ADT-aware datastores.

## Transaction boosting

To ensure serializability, executing transactions requires some form of concurrency control. A common approach is two-phase-locking (2PL), where a transaction must acquire locks on all the records it is accessing before performing any irreversible changes. Typically these locks are reader/writer locks, so multiple read operations can be executed concurrently in different transactions. This means that transactions need not abort if they are reading a record already being read by other transactions, but writes cause transactions to abort. This behavior is the same with optimistic concurrency control.

*Abstract locks* [@Ni:07] generalize the notion of reader/writer locks to any operations which can logically run concurrently on the same record. Using an abstract lock for an ADT, the concurrency control mechanism can allow operations that commute to run concurrently. There is a similar way to represent this behavior in optimistic concurrency control schemes, too.

If we can express the *commutativity* of our ADT operations to the concurrency control system via abstract locks, then it could avoid aborting many more transactions, especially on highly contended records. This idea of raising the level of reasoning to higher-level ADT operations is known as *transaction boosting* [@Herlihy:PPoPP08] in the transactional memory community.

## Combining

Another useful property on operations is *associativity*. If we think of *commutativity*, used in boosting above, as allowing operations to be executed in a different order on a given record, then *associativity* allows us to merge some of those operations together *before* applying them to the record. This technique, known as *combining* [@flatCombining; @yew:combining-trees; @funnels] can drastically reduce contention on shared data structures and improve performance in situations where applying the combined operation is cheaper than applying the operations one-by-one. In distributed settings this is very often the case [@flat-combining-pgas13], especially when it allows eliminating a round-trip from the client to the datastore.

Where to execute combiners is an interesting question because finding combining opportunities requires collecting operations from multiple concurrent clients. Doing combining on the client side is unlikely to be beneficial since there are very few operations. Similarly, performing combining at the datastore shard where the operations would be applied anyway is unlikely to show any savings. However, a typical configuration for routing traffic from a service's frontend to the datastore backend involves load-balancers and proxies like Nginx [@nginx]. As these proxies arbitrate large numbers of client connections, they would be in a great place to perform combining between datastore operations. [todo: possibly move this paragraph to evaluation section]

## Phasing

Sometimes the order that operations happen to arrive causes problems when applying the two former optimizations. In particular, leveraging commutativity only really works if many of the operations that commute with each other arrive together. If they are interleaved in time with operations they do not commute with, then all of that cleverness is for naught.

Borrowing the term from recent work on *phase reconciliation* [@Narula:OSDI14], *phasing* is the idea of enforcing order on operations that are otherwise unordered to improve the efficiency of execution. Using the commutativity specification for a data type, discussed in greater detail in [#sec-commutativity-spec], the *phaser* is responsible for sorting in-flight operations into *modes* which can run together. This can delay some operations longer than they may have executed before if they are forced back to a later phase, and fairness is of course a concern as well. However, the goal is to reduce the amount of unnecessary conflicts between operations, reducing the number of aborts and reducing the latency of operations overall.
