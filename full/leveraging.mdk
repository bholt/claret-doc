# Leveraging data types {#sec-leveraging}

~ Fig { #fig-adt-bid caption="Claret programming model example showing the ADT for a scored set and how it can be used to implement the Bid transaction from [#fig-levels]." }
![Example ADT version of Bid transaction](fig/adt-bid.pdf){.onecol}
~

<!--
- adts hide their implementation, allow reasoning about abstract behavior
- if abstract view allows reordering, then concurrency control can take advantage of that
- use adts to express application-level properties
 -->

<!-- At the application level, actions like `Bid` may obviously commute with one another, but once these actions have been broken down into individual `put` and `get` operations, that knowledge is lost. The datastore no longer has any way to see that which reorderings are allowable, so adding multiple items to a `Set`, for instance, just looks like a series of writes with some implicit order. We need some way to communicate these higher-level properties about the application to the storage and execution system. -->

<!-- Abstract data types ... because they allow reasoning about the abstract behavior -->
<!-- are a natural and succinct way to implement applications.  -->

<!-- As was alluded to earlier, data types hold the key. Specifically, the notion of  -->




In Claret, programmers express application-level semantics through ADTs. We know that the `Bid` transactions in [#fig-levels] should commute somehow, but how do we implement them so that this is true? We wish to track all the bids for each item and determine the maximum bid. The solution is to choose data types that capture the behavior we want. A *scored set* data type allows the items in the set to be ranked according to a score associated with each item, which works perfectly for tracking an item's bids, shown in [#fig-adt-bid]. Because `zset.add` operations commute, `Bid` transactions will now also commute.

Abstract data types decouple their abstract behavior from their low-level concrete implementation.
Abstract operations can have properties such as commutativity, associativity, or monotonicity, which tell users how they can be reordered or executed concurrently, while the concrete implementation takes care of making it so by performing the necessary synchronization. 

Knowledge of these properties of operations can be used by the datastore in many ways to improve performance. First, we will show how commutativity can be used in the concurrency control system to avoid false conflicts and ordering constraints. Then we will give an example of how associativity can be applied to reduce the load on the datastore.

## Transaction boosting

To ensure serializability, executing transactions requires some form of concurrency control. A common approach is two-phase-locking (2PL), where a transaction must acquire locks on all the records it will access before performing any irreversible changes. Typically these are reader/writer locks, so multiple read operations can be executed concurrently in different transactions. This means that transactions need not abort if they are reading a record already being read by other transactions, but writes cause transactions to abort.

*Abstract locks* [@Ni:07] generalize the notion of reader/writer locks to any operations which can logically run concurrently on the same record. With an abstract lock for an ADT, the concurrency control system can allow any operations that commute to hold the lock at the same time. For `zset`, `add` operations can all hold the lock at the same time, but reading operations such as `size` must wait. The same idea can be applied to optimistic concurrency control: operations only cause conflicts if the abstract lock doesn't allow them to execute concurrently with other outstanding operations.

Using abstract locks to improve concurrency in transactions is known as *transaction boosting* [@Herlihy:PPoPP08] in the transactional memory community. 
But this technique is of possibly even greater importance to distributed transactions because the performance of distributed transactions is highly dependent on how long locks are held for.
Waiting for one lock causes a cascade of waiting as others wait on the locks the blocked transaction holds. Every operation that can share an abstract lock reduces the time the rest of its locks are held for, which can have a big impact on performance.
In OCC-based systems, boosting reduces the abort rate because fewer operations conflict with one another.

Boosting is especially important for highly contended records. In the case of auctions, when a particularly hot auction is near closing time, it can expect to receive a huge number of bids. If all of the bids conflict with each other and serialize, it may cause some of them to not complete in time. However, if the bids are represented using a `zset`, then they naturally commute, the transactions can execute concurrently, and everyone gets to place their bids.

## Phasing

Sometimes the order that operations happen to arrive causes problems when applying the two former optimizations. In particular, leveraging commutativity only really works if many of the operations that commute with each other arrive together. If they are interleaved in time with operations they do not commute with, then all of that cleverness is for naught.

Borrowing the term from recent work on *phase reconciliation* [@Narula:OSDI14], *phasing* is the idea of enforcing order on operations that are otherwise unordered to improve the efficiency of execution. Using the commutativity specification for a data type, discussed in greater detail in [#sec-commutativity-spec], the *phaser* is responsible for sorting in-flight operations into *modes* which can run together. This can delay some operations longer than they may have executed before if they are forced back to a later phase, and fairness is of course a concern as well. However, the goal is to reduce the amount of unnecessary conflicts between operations, reducing the number of aborts and reducing the latency of operations overall.

## Combining

Another useful property on operations is *associativity*. If we think of *commutativity*, used in boosting above, as allowing operations to be executed in a different order on a given record, then *associativity* allows us to merge some of those operations together *before* applying them to the record. This technique, known as *combining* [@flatCombining; @yew:combining-trees; @funnels] can drastically reduce contention on shared data structures and improve performance in situations where applying the combined operation is cheaper than applying the operations one-by-one. In distributed settings this is very often the case [@flat-combining-pgas13], especially when it allows eliminating a round-trip from the client to the datastore.

Where to execute combiners is an interesting question because finding combining opportunities requires collecting operations from multiple concurrent clients. Doing combining on the client side is unlikely to be beneficial since there are very few operations. Similarly, performing combining at the datastore shard where the operations would be applied anyway is unlikely to show any savings. However, a typical configuration for routing traffic from a service's frontend to the datastore backend involves load-balancers and proxies like Nginx [@nginx]. As these proxies arbitrate large numbers of client connections, they would be in a great place to perform combining between datastore operations. [todo: possibly move this paragraph to evaluation section]
