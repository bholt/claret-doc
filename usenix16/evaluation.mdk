# Evaluation {#sec-eval}

To understand the potential performance impact of the optimizations enabled by ADTs, we built a prototype in-memory ADT-store in C++, dubbed Claret. Our design follows the architecture previously described in [#system-diagram], with keys distributed among shards by consistent hashing, though related keys may be co-located using tags as in Redis. Records are kept in their in-memory representations, avoiding serialization. Clients are multi-threaded, to model frontend servers handling many concurrent end-user requests, and use ethernet sockets and protocol buffers to communicate with the data servers.

As discussed before, Claret uses two-phase locking (2PL) to isolate transactions.
The baseline uses reader/writer locks. When boosting is enabled, these are replaced with abstract locks. By default, operations retry whenever they fail to acquire a lock; with phasing, replies are sent whenever the lock is finally acquired, so retries are only used to resolve deadlocks or lost packets.

Our prototype does not provide fault-tolerance or durability. These could be implemented by replicating shards or logging operations to persistent storage.
Synchronizing with replicas or the filesystem is expected to increase the time spent holding locks. In 2PL, the longer locks are held, lower throughput and higher latency are expected. Claret increases the number of clients which can simultaneously hold locks, so adding fault tolerance would be expected to reinforce our findings.

In our evaluation, we wish to understand the impact our ADT optimizations have on throughput and latency under various contention scenarios. First, we use a microbenchmark to directly tune contention by varying operation mix and key distribution. We then move on to explore scenarios modeling real-world contention in two benchmarks â€“ Rubis: an online auction service, and Retwis: a Twitter clone.

The following experiments are run un-virtualized on a RedHat Linux cluster. Each node has dual 6-core 2.66 GHz Intel Xeon X5650 processors with 24 GB of memory, connected by a 40Gb Mellanox ConnectX-2 InfiniBand network, but using normal TCP-over-Infiniband rather than specializing to leverage this hardware's RDMA support. Experiments are run with 4 single-threaded shards running on 4 different nodes, with 4 clients running on other nodes with variable numbers of threads. Average round-trip times between nodes for UDP packets are 150 &mu;s, with negligible packet loss.

## Raw Operation Mix

~ Fig { #plot-rawmix caption="Performance of raw mix workload (50% read, zipf: 0.6) with increasing number of clients, plotted as throughput versus latency. Boosting (abstract locks) eliminate conflicts between `add`s, and phasing helps operations acquire locks in an efficient order, resulting in a 2.6x improvement in throughput, 63% of the throughput without transactions." }
![Raw mix throughput versus latency](plots/rawmix-tput-vs-lat.pdf){ .onecol }
~

<!--
~ Fig { #plot-rawmix-retries caption="Transaction retries for Rawmix workload with 384 clients. Throughput is largely determined by retries. Phasing, boosting, and combining all reduce retries in different ways." }
![Raw mix throughput versus latency](plots/rawmix-retries.pdf){ .onecol }
~
-->
This microbenchmark performs a random mix of operations, similar to YCSB or YCSB+T [@YCSB;@YCSB-T], that allows us to explicitly control the degree of contention.
Each transaction executes a fixed number of operations (4), randomly selecting either a read operation (`set.size`), or a commutative write operation (`set.add`), and keys selected randomly with a Zipfian distribution from a pool of 10,000 keys. 
By varying the percentage of `add`s, we control the number of potential conflicting operations. Importantly, `add`s commute with one another, but not with `size`, so even with boosting, conflicts remain.
The Zipf parameter, &alpha;, determines the shape of the distribution; a value of 1 corresponds to Zipf's Law, lower values are shallower and more uniform, higher values more skewed. YCSB sets &alpha; near 1; we explore a range of parameters.

We start with a 50% read, 50% write workload and a modest zipfian parameter of 0.6, and vary the number of clients. [#plot-rawmix] shows a throughput versus latency plot with lines showing each condition as we vary the number of clients (from 8 to 384). The baseline, using traditional r/w locks, reaches peak throughput with few clients before latencies spike; throughput suffers as additional clients create more contention. Abstract locks (*boosting*) expose more concurrency, increasing peak throughput. Adding phasing (dashed lines) results in significant improvement in peak throughput. This is because phasing ensures that all transactions get their turn while also improving the chances of commuting.

The dotted pink line in [#plot-rawmix] shows performance of the same workload with operations executed independently, without transactions (though performance is still measured in terms of the "transactions" of groups of 4 operations). These operations execute immediately on the records in a linearizable [@Herlihy:90] fashion without locks. This serves as a reasonable upper bound on the throughput possible with our servers.
Claret's transactions achieve 63% of that throughput on this workload.

~ Fig { #plot-mix caption="*Peak throughput, varying operation mix.* Boosting is increasingly important with a higher fraction of adds. Phasing is essential for any mixed workload." }
![Varying mix](plots/rawmix-mix.pdf){ .onecol }
~

**Varying operation mix.** [#plot-mix] shows throughput as we vary the percentage of commutative write operations (`add`), with keys selected with a modest 0.6 zipfian distribution.
Boosting becomes more important as the fraction of commutative `add`s increases. 
Phasing has a significant performance impact for any mixed workload, as it helps commutative operations run concurrently; tracing the execution, we observed that records regularly alternated between `add` and `read` phases.
Combining shows a modest improvement for all workloads, even for read-only and write-only, because it occasionally allows transactions to share locks (and the result of reads) without burdening the server.

~ Fig { #plot-zipf caption="*Peak throughput, varying key distribution.* Higher Zipf parameter results in greater skew and contention; boosting and phasing together expose concurrency. At extreme skew, combining reduces load on hot records, which our non-transactional mode cannot do." }
![Varying zipfian](plots/rawmix-zipf.pdf){ .onecol }
~

**Varying key distribution.** 
[#plot-zipf] shows throughput with a 50/50 operation mix, controlling contention by adjusting the zipfian skew parameter used to choose keys.
At low zipfian, the distribution is mostly uniform over the 10,000 keys, so most operations are concurrent simply because they fall on different keys, and Claret shows little benefit. As the distribution becomes more skewed, transactions contend on a smaller set of popular records. With less inter-record concurrency, we rely on abstract locks (boosting) to expose concurrency within records.

At high skew, there is a steady drop in performance simply due to serializing operations on the few shards unlucky enough to hold the popular keys.
However, skew increases the chance of finding operations to combine with, so combining is able to offload significant load from the hot shards.
Our implementation of combining requires operations to be split into the acquire and commit phases, so it cannot be used without transactions. Though distributions during normal execution are typically more moderate, extreme skew models the behavior during exceptional situations like BuzzFeed's viral dress.

## RUBiS

~ Fig { #plot-rubis caption="Throughput of Rubis. Contention between bids on popular auctions, especially close to their closing time, causes performance to drop for r/w locks, but bids commute, so boosting is able to maintain high throughput even under heavy bidding." }
![Rubis](plots/rubis-tput.pdf){ .onecol }
~

~ Fig { #plot-rubis-conflicts caption="Breakdown of conflicts between Rubis transactions (minor contributors omitted) with 256 clients on bid-heavy workload (averaged). As predicted by [#fig-rubis-conflicts], boosting drastically reduces Bid-Bid conflicts, and phasing drastically reduces the remaining conflicts." }
![Rubis](plots/rubis-conflicts.pdf){ .onecol width=3.1in }
~

The RUBiS benchmark [@Amza:02] imitates an online auction service like the one described in [#sec-realworld]. The 8 transaction types and their frequencies are shown in [#tbl-workloads]; `ViewAuction` and `Bid` dominate the workload. The benchmark specifies a workload consisting of a mix of these transactions and the average bids per auction. However, the distribution of bids (by item and time) was unspecified.

Our implementation models the bid distributions observed by subsequent studies [@Akula:04;@Akula:07], with bids per item following a power law and the frequency of bids increasing exponentially at the end of an auction.
Otherwise, we follow the parameters specified in [@Amza:02]: 30,000 items, divided into 62 regions and 40 categories, with an average of 10 bids per item, though in our case this is distributed according to a zipfian with $\alpha = 1$.

[#plot-rubis] shows results for two different workloads: read-heavy and bid-heavy. 
In the read-heavy workload, bids do not often come in at a high enough rate to require commutativity, so phasing alone suffices. However, during heavy bidding times, commutativity is essential: Claret maintains nearly the same throughput in this situation as the read-heavy workload, roughly 2x better than r/w locks and 68% of non-transactional performance. Considering the importance of getting bids correct, this seems an acceptable tradeoff.

Prior work [@Akula:04] observed contention spikes when popular auctions closed, leading to momentary performance drops which could be noticeable to even to users elsewhere on the site.
Analyzing a trace of throughput over time, we observed that boosting significantly reduced variability in throughput by 2x. The minimum throughput (over 5-second windows) was also increased from 9k txn/s for the baseline with phasing to 12k.

[#fig-rubis-conflicts] showed which conflicts should be affected by Claret; in [#plot-rubis-conflicts] we validate those predictions by plotting the actual number of conflicts for the most significant edges in the conflict graph. Using a log scale, it is apparent that boosting all but eliminates Bid-Bid conflicts, but Bid-View conflicts have gone up; now that there are more bids, the chances of conflicting with `ViewAuction` have increased. The introduction of phasing and combining eliminate much of the remaining conflicts.

Overall, we can see that boosting and phasing are crucial to achieving reasonable transaction performance in Rubis even during heavy bidding. If an auction service is unable to keep up with the rate of bidding, it will result in a loss of revenue and a lack of trust from users, so a system like Claret could prove invaluable to them.

## Retwis

~ Fig { #plot-retwis caption="Throughput of Retwis. Boosting is essential during heavy posting because network effects lead to extreme contention on some records. Non-phasing results elided due to too many failed transactions." }
![Retwis](plots/retwis-tput.pdf){ .onecol }
~

~ Tab { #tbl-workloads caption="Transaction mix for benchmark workloads." }
![Workloads](fig/workloads.pdf){ .onecol width=3.1in }
~

Retwis is a simplified Twitter clone designed originally for Redis [@redis]. Data structures such as lists and sets are used track each user's followers and posts and keep a materialized up-to-date timeline for each user (as a `zset`). It is worth noting that in our implementation, `Post` and `Repost` are each a single transaction, including appending to all followers' timelines, but when viewing timelines, we load each post in a separate transaction.

Retwis doesn't specify a workload, so we simulate a realistic workload using a synthetic graph with power-law degree distribution and a simple user model.
We use the Kronecker graph generator from the Graph 500 benchmark [@graph500], which is designed to produce the same power-law degree distributions found in natural graphs. These experiments generate a graph with approximately 65,000 users and an average of 16 followers per user.

Our simple model of user behavior determines when and which posts to repost. 
After each timeline action, we rank the posts by how many reposts they already have and repost the most popular ones with probability determined by a geometric distribution. The resulting distribution of reposts follows a power law, approximating the viral propagation effects observed in real social networks.

[#plot-retwis] shows throughput on two workloads, listed in [#tbl-workloads]. The read-heavy workload models steady-state Twitter traffic, while the post-heavy workload models periods of above-average posting, such as during live events. 
We only show the results with phasing because the non-phasing baseline had too many failed transactions. On the read-heavy workload, r/w locks are able to keep up reasonably well; after all, reading timelines is easy as long as they do not change frequently.
However, the post-heavy workload shows that when contention increases, the performance of r/w locks falls off much more drastically than with boosting. Combining even appears to pay off when there are enough clients to find matches.

Unlike auctions, many situations in Twitter are tolerant of minor inconsistencies, making it acceptable to implement without transactions in order to aid scalability.
This tradeoff is clear when performance is as flat as the baseline performance is in these plots. However, with Claret's optimizations, it is able to achieve up to 82% of the non-transactional performance. In situations where inconsistent timelines are more likely to be noticed, such as ongoing conversations, it may be worth paying this much smaller overhead.

**Evaluation summary.** We find that leveraging commutativity via boosting and phasing is clearly beneficial under all of our simulated scenarios, showing greater benefit under more extreme contention resulting from high skew or heavy writing. 
Combining appears mostly ineffectual in our benchmarks, but does not hinder performance. In the most dire circumstances of extreme contention, having combining as an optional release valve for offloading work is useful.
Moreover, these improvements come with a programming model largely identical to Redis's, which is sufficient for many applications that only require simple ADTs already built into Redis.
More complex applications will likely require more complex ADTs to expose the available concurrency, but Claret's interfaces allow it to be extended with custom ADTs, though it may be for experts only.
