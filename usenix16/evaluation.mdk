# Evaluation {#sec-eval}

To understand the potential performance impact of the optimizations enabled by ADTs, we built a prototype in-memory ADT-store in C++, dubbed Claret. Our design follows the architecture previously described in [#system-diagram], with keys distributed among shards by consistent hashing, though related keys may be co-located using tags as in Redis. Records are kept in their in-memory representations, avoiding serializing. Clients are multi-threaded, modeling frontend servers handling many concurrent end-user requests, and use ethernet sockets and protocol buffers to communicate with the data servers.

As discussed before, Claret uses two-phase locking (2PL) to isolate transactions.
The baseline uses reader/writer locks. When boosting is enabled, these are replaced with abstract locks. By default, operations retry whenever they fail to acquire a lock; with phasing, replies are sent whenever the lock is finally acquired, so retries are only used to resolve deadlocks or lost packets.

Our prototype does not provide fault-tolerance or durability. These could be implemented by replicating shards or logging operations to persistent storage.
Synchronizing with replicas or the filesystem is expected to increase the time spent holding locks. In 2PL, the longer locks are held, lower throughput and higher latency are expected. Claret increases the number of clients which can simultaneously hold locks, so adding fault tolerance would be expected to reinforce our findings.

In our evaluation, we wish to understand the impact our ADT optimizations have on throughput and latency under various contention scenarios. First, we use a microbenchmark to directly tune contention by varying operation mix and key distribution. We then move on to explore scenarios modeling real-world contention in two benchmarks â€“ Rubis: an online auction service, and Retwis: a Twitter clone.

The following experiments are run un-virtualized on a RedHat Linux cluster. Each node has dual 6-core 2.66 GHz Intel Xeon X5650 processors with 24 GB of memory, connected by a 40Gb Mellanox ConnectX-2 InfiniBand network, but using normal TCP-over-Infiniband rather than specializing to leverage this hardware's RDMA support. Experiments are run with 4 single-threaded shards running on 4 different nodes, with 4 clients running on other nodes with variable numbers of threads. Average round-trip times between nodes for UDP packets are 150 &mu;s, with negligible packet loss.

## Raw Operation Mix

~ Fig { #plot-rawmix caption="Performance of raw mix workload (50% read, zipf: 0.6) with increasing number of clients, plotted as throughput versus latency. Boosting (abstract locks) eliminate conflicts between `add`s, and phasing helps operations acquire locks in an efficient order, resulting in a 2.6x improvement in throughput, 63% of the throughput without transactions." }
![Raw mix throughput versus latency](plots/rawmix-tput-vs-lat.pdf){ .onecol }
~

<!--
~ Fig { #plot-rawmix-retries caption="Transaction retries for Rawmix workload with 384 clients. Throughput is largely determined by retries. Phasing, boosting, and combining all reduce retries in different ways." }
![Raw mix throughput versus latency](plots/rawmix-retries.pdf){ .onecol }
~
-->
This microbenchmark performs a random mix of operations, similar to YCSB or YCSB+T [@YCSB;@YCSB-T], that allows us to explicitly control the degree of contention.
Each transaction executes a fixed number of operations (4), randomly selecting either a read operation (`set.size`), or a commutative write operation (`set.add`), and keys selected randomly with a zipfian distribution from a pool of 10,000 keys. 
By varying the percentage of `add`s, we control the number of potential conflicting operations. Importantly, `add`s commute with one another, but not with `size`, so even with boosting, conflicts remain.
The zipfian parameter, &alpha;, determines the shape of the distribution; a value of 1 corresponds to Zipf's Law, lower values are shallower and more uniform, higher values more skewed. YCSB sets &alpha; near 1; we explore a range of parameters.

We start with a 50% read, 50% write workload and a modest zipfian parameter of 0.6, and vary the number of clients. [#plot-rawmix] shows a throughput versus latency plot with lines showing each condition as we vary the number of clients (from 8 to 384). The baseline, using traditional r/w locks, reaches peak throughput with few clients before latencies spike; throughput suffers as additional clients create more contention. Abstract locks (*boosting*) expose more concurrency, increasing peak throughput. Adding phasing (dashed lines) results in significant improvement in peak throughput. This is because phasing ensures that all transactions get their turn while also improving the chances of commuting.

The dotted pink line in [#plot-rawmix] shows performance of the same workload with operations executed independently, without transactions (though performance is still measured in terms of the "transactions" of groups of 4 operations). These operations executing immediately on the records in a linearizable [@Herlihy:90] fashion without locks. This serves as a reasonable upper bound on the throughput possible with our servers.
Claret's transactions achieve 63% of that throughput on this workload.

~ Fig { #plot-mix caption="*Peak throughput, varying operation mix.* Boosting is increasingly important with a higher fraction of adds. Phasing is essential for any mixed workload." }
![Varying mix](plots/rawmix-mix.pdf){ .onecol }
~

**Varying operation mix.** [#plot-mix] shows throughput as we vary the percentage of commutable write operations (`add`).
From this experiment, we can conclude that phasing is useful even for relatively light amounts of writes because it helps ensure that `add` operations get their turn amidst all the read operations, and phasing is made even more useful with abstract locks (boosting), where sets regularly alternate between `add` and `read` phases. Moreover, we observe that combining can benefit even read-only or write-only workloads because it allows transactions to share results, reducing load on the servers.

~ Fig { #plot-zipf caption="*Peak throughput, varying key distribution.* Higher zipfian results in greater skew and contention; boosting and phasing together expose concurrency. At extreme skew, combining reduces load on hot records, which our non-transactional mode cannot do." }
![Varying zipfian](plots/rawmix-zipf.pdf){ .onecol }
~

**Varying key distribution.** 
[#plot-zipf] shows throughput with a 50/50 operation mix, controlling contention by adjusting the zipfian skew parameter used to choose keys.
At low zipfian, the distribution is mostly uniform over the 10,000 keys, so most operations are concurrent simply because they fall on different keys, and Claret shows little benefit. As the distribution becomes more skewed, transactions contend on a smaller set of popular records. With less inter-record concurrency, we rely on abstract locks (boosting) to expose concurrency within records.

At high skew, there is a steady drop in performance simply due to serializing operations on the few shards unlucky enough to hold the popular keys.
However, skew increases the chance of finding operations to combine with, so combining is able to offload significant load from the hot shards.
Our implementation of combining requires operations to be split into the acquire and commit phases, so it cannot be used without transactions. Though distributions during normal execution are typically more moderate, extreme skew models the behavior during exceptional situations like BuzzFeed's viral dress.

Now that we have looked at various degrees of artificial contention, our next benchmarks exhibit contention modeled on real-world scenarios.

## RUBiS

~ Fig { #plot-rubis caption="Throughput of Rubis. Contention between bids on popular auctions, especially close to their closing time, causes performance to drop for r/w locks, but bids commute, so boosting is able to maintain high throughput even under heavy bidding." }
![Rubis](plots/rubis-tput.pdf){ .onecol }
~

~ Fig { #plot-rubis-conflicts caption="Breakdown of conflicts between Rubis transactions (minor contributors omitted) with 256 clients on bid-heavy workload (averaged). As predicted by [#fig-rubis-conflicts], boosting drastically reduces Bid-Bid conflicts, and phasing drastically reduces the remaining conflicts." }
![Rubis](plots/rubis-conflicts.pdf){ .onecol width=3.1in }
~

The RUBiS benchmark [@Amza:02] imitates an online auction service like the one described in [#sec-realworld]. The 8 transaction types and their frequencies are shown in [#tbl-workloads]; `ViewAuction` and `Bid` dominate the workload. The benchmark specifies a workload consisting of a mix of these transactions and the average bids per auction. However, the distribution of bids (by item and time) was unspecified.

Our implementation models the bid distributions observed by subsequent studies [@Akula:04;@Akula:07], with bids per item following a power law and the frequency of bids increasing exponentially at the end of an auction.
Otherwise, we follow the parameters specified in [@Amza:02]: 30,000 items, divided into 62 regions and 40 categories, with an average of 10 bids per item, though in our case this is distributed according to a zipfian with $\alpha = 1$.

[#plot-rubis] shows results for two different workloads: read-heavy (10% bid transactions), and bid-heavy (50% bid). 
For the read-heavy workload, phasing provides reasonable performance, as bids do not often come in at a high enough rate to require commutativity. However, during high bidding times, modeled by our bid-heavy workload, it is essential to allow bids to commute. Claret maintains nearly the same throughput in this situation as the read-heavy workload, roughly 2x better than r/w locks and 68% of non-transactional performance. Considering the importance of getting bids correct, this seems like an acceptable tradeoff.

Prior work [@Akula:04] observed contention spikes when popular auctions closed, leading to momentary performance drops which could be noticeable to even to users elsewhere on the site.
Analyzing a trace of throughput over time, we observed that boosting significantly reduced variability in throughput by 2x. The minimum throughput (over 5s windows) was also increased from 9k txn/s for the baseline with phasing to 12k for boosting.

[#fig-rubis-conflicts] showed which conflicts should be affected by Claret; in [#plot-rubis-conflicts] we validate those predictions by plotting the actual number of conflicts for the most significant edges in the conflict graph. Using a log scale, it is apparent that boosting all but eliminates Bid-Bid conflicts, but Bid-View conflicts have gone up; now that there are more bids, the chances of conflicting with `ViewAuction` have increased. The introduction of phasing and combining eliminate much of the remaining conflicts.

Overall, we can see that boosting and phasing are crucial to achieving reasonable transaction performance in Rubis even during heavy bidding. If an auction service is unable to keep up with the rate of bidding, it will result in a loss of revenue and a lack of trust from users, so a system like Claret could prove invaluable to them.

## Retwis

~ Fig { #plot-retwis caption="Throughput of Retwis. Boosting is essential during heavy posting because network effects lead to extreme contention on some records. Non-phasing results elided due to too many failed transactions." }
![Retwis](plots/retwis-tput.pdf){ .onecol }
~

~ Tab { #tbl-workloads caption="Transaction mix for benchmark workloads." }
![Workloads](fig/workloads.pdf){ .onecol width=3.1in }
~

Retwis is a simplified Twitter clone designed originally for Redis [@redis]. Data structures such as lists and sets are used track each user's followers and posts and keep a materialized up-to-date timeline for each user (as a `zset`). It is worth noting that in our implementation, `Post` and `Repost` are executed as transactions, including when they append to all their followers' timelines, but when timelines are read, loading each tweet is done in a separate transaction.
Retwis doesn't specify a workload, so we simulate a realistic workload using a synthetic graph with power-law degree distribution and a simple user model.

We use the Kronecker graph generator from the Graph 500 benchmark [@graph500], which is designed to produce the same power-law degree distributions found in natural graphs. These experiments generate a graph with approximately 65,000 users and an average of 16 followers per user (out-degree follows a power law).

Our simple model of user behavior determines when and which posts to repost. 
After each timeline action, we rank the posts by how many reposts they already have and repost the most popular ones with probability determined by a geometric distribution. The resulting distribution of reposts follows a power law, approximating the viral propagation effects observed in real social networks.

[#plot-retwis] shows throughput on two workloads, listed in [#tbl-workloads]. Read-heavy models steady-state Twitter traffic, while the post-heavy workload models periods of above-average posting, such as during live events. 
We show only the results with phasing because the non-phasing baseline had too many transactions it was unable to complete. On the read-heavy workload, which models steady-state Twitter traffic, r/w locks are able to keep up reasonably well; after all, reading timelines is easy as long as they do not change frequently.
However, the post-heavy workload shows that when contention increases, the performance of r/w locks falls off much more drastically than with boosting. Combining even appears to pay off when there are enough clients to find matches.

Unlike auctions, Twitter is typically characterized as a non-transactional application because users are willing to tolerate minor inconsistencies. This tradeoff is clear when performance is as flat as the reader/writer performance is in these plots. However, with Claret's optimizations, it is able to achieve up to 82% of the non-transactional performance. This is largely due to the heavy amount of reads, even for our post-heavy workload. In situations where inconsistent timelines are more likely to be noticed, such as ongoing conversations, it may be worth paying this smaller overhead.

Overall, our evaluation finds that leveraging commutativity via boosting and phasing is clearly beneficial under all of our simulated scenarios, showing greater benefit under more extreme contention resulting from high skew or heavy writing. This is encouraging because the applications used simple ADTs that could be expected to be in a default library, and close to what Redis supports, which means the implementations are similar to what would be expected for Redis. 
More complex applications will likely require more complex ADTs to expose their concurrency; luckily Claret's abstract lock, phaser, and combiner interfaces allow it to be extended with custom ADTs.
