# Evaluation {#sec-eval}

To understand the potential performance impact of the optimizations enabled by ADTs, we built a prototype in-memory ADT-store in C++, dubbed Claret. Our design follows the architecture previously described in [#system-diagram], with keys distributed among shards by consistent hashing, though related keys may be co-located using tags as in Redis. Records are kept in their in-memory representations as in Redis to avoid serializing and deserializing except for operations from the clients. Clients are multi-threaded, modeling frontend servers handling many concurrent end-user requests, and use ethernet sockets and protocol buffers to communicate with the data servers.

As discussed before, Claret uses a standard two-phase locking (2PL) scheme to isolate transactions.
The baseline uses reader/writer locks which allow any number of read-only operations in parallel, but requires an exclusive lock for operations that modify the record. When boosting is enabled, these are replaced with abstract locks, which allow any operations that commute with one another to hold the lock at the same time. By default, operations retry whenever they fail to acquire a lock; with phasing, replies are sent whenever the lock is finally acquired, so retries are only used to resolve deadlocks or lost packets.

This prototype does not currently provide fault-tolerance or durability. These could be implemented by replicating shards or logging operations to persistent storage.
Synchronizing with replicas or the filesystem is expected to increase the time spent holding locks. In 2PL, the longer locks are held, lower throughput and higher latency are expected. Claret increases the number of clients which can simultaneously hold locks, so providing fault tolerance should be expected to exaggerate the results we obtained.

In our evaluation, we wish to understand the impact our ADT optimizations have on throughput and latency under various contention scenarios. First, we use a microbenchmark to directly tune contention by varying operation mix and key distribution. We then move on to explore scenarios modeling real-world contention in two benchmarks â€“ Rubis: an online auction service, and Retwis: a Twitter clone.

The following experiments are run on our own cluster in RedHat Linux, un-virtualized. Each node has dual 6-core 2.66 GHz Intel Xeon X5650 processors with 24 GB of memory, connected by a 40Gb Mellanox ConnectX-2 InfiniBand network, but using normal TCP-over-Infiniband rather than specializing to leverage this hardware's RDMA support. Experiments are run with 4 single-threaded shards running on 4 different nodes, with 4 clients running on other nodes with variable numbers of threads. Average round-trip times between nodes for UDP packets are 150 &mu;s, with negligible packet loss.

## Raw Operation Mix

~ Fig { #plot-rawmix caption="Performance of raw mix workload (50% read, zipf: 0.6) with increasing number of clients, plotted as throughput versus latency. Boosting (abstract locks) eliminate conflicts between `add`s, and phasing helps operations acquire locks in an efficient order, resulting in a 2.6x improvement in throughput, 63% of the throughput without transactions." }
![Raw mix throughput versus latency](plots/rawmix-tput-vs-lat.pdf){ .onecol }
~

<!--
~ Fig { #plot-rawmix-retries caption="Transaction retries for Rawmix workload with 384 clients. Throughput is largely determined by retries. Phasing, boosting, and combining all reduce retries in different ways." }
![Raw mix throughput versus latency](plots/rawmix-retries.pdf){ .onecol }
~
-->
This microbenchmark performs a random mix of operations, similar to YCSB or YCSB+T [@YCSB;@YCSB-T], that allows us to explicitly control the degree of contention.
Each transaction executes a fixed number of operations (4), randomly selecting either a read operation (`set.size`), or a commutative write operation (`set.add`), and keys selected randomly with a zipfian distribution from a pool of 10,000 keys. 
By varying the percentage of `add`s, we control the number of potential conflicting operations. Importantly, `add`s commute with one another, but not with `size`, so even with boosting, conflicts remain.
The zipfian parameter, &alpha;, determines the shape of the distribution; a value of 1 corresponds to Zipf's Law, lower values are shallower and more uniform, higher values more skewed. YCSB sets &alpha; near 1; we explore a range of parameters in [#plot-zipf].

 <!-- but other prior work [@Breslau:99] found that lower values of &alpha;, ranging from 0.64 to 0.83, better characterized the workloads they studied. We explore a range of &alpha;'s in [#plot-zipf], but default to 0.6 elsewhere to be conservative, since Claret's optimizations perform better with more skew. -->

First, we start with a 50% read, 50% write workload and a modest zipfian parameter of 0.6, and vary the number of clients. [#plot-rawmix] shows a throughput versus latency plot with lines showing each condition as we vary the number of clients (from 8 to 384). We see immediately that the baseline, using traditional r/w locks, reaches peak throughput with few clients and then latencies go up and throughput suffers as more clients create more contention. Using abstract locks (*boosting*) to expose more concurrency increases peak throughput somewhat. Using phasing in addition (dashed lines) results in significant throughput improvement and lower latency. This is because phasing ensures that all transactions get their turn while also improving the chances that operations will commute.

The dotted pink line in [#plot-rawmix] shows performance of the same workload where operations are executed independently, without transactions (though performance is still measured in terms of the "transactions" of groups of 4 operations). These operations execute without conflicting, since they do not acquire locks, instead executing immediately on the records in a linearizable [@Herlihy:90] fashion. This serves as a reasonable upper bound on the throughput possible with our servers.
Claret achieves 63% of that upper bound on throughput on this workload, though of course with strong consistency.

### Varying operation mix

~ Fig { #plot-mix caption="*Peak throughput, varying operation mix.* With even just 20% `add`s, throughput plummets without phasing, which helps `adds` get a chance to run between all the reads. With higher `add` ratios, boosting becomes important." }
![Varying mix](plots/rawmix-mix.pdf){ .onecol }
~

[#plot-mix] shows throughput as we vary the percentage of commutable write operations (`add`).
From this experiment, we can conclude that phasing is useful even for relatively light amounts of writes because it helps ensure that `add` operations get their turn amidst all the read operations, and phasing is made even more useful with abstract locks (boosting), where sets regularly alternate between `add` and `read` phases. Moreover, we observe that combining can benefit even read-only or write-only workloads because it allows transactions to share results, reducing load on the servers.

### Varying key distribution

~ Fig { #plot-zipf caption="*Peak throughput, varying key distribution.* Higher zipfian results in more skew and greater contention; boosting is essential to expose concurrency on hot records. At extreme skew, combining becomes much more likely to occur, while our non-transactional mode is unable to benefit from it." }
![Varying zipfian](plots/rawmix-zipf.pdf){ .onecol }
~

Our final way to control contention is to vary the zipfian parameter used to select keys; [#plot-zipf] shows throughput results with 50% `add`s. 
At low zipfian, the distribution is mostly uniform over the 10,000 keys, so most operations are concurrent simply because they fall on different keys, though because of the significantly write-heavy workload, Claret's optimizations do help modestly. As the zipfian distribution becomes more skewed, transactions contend more often on fewer, more popular, records. With less inter-record concurrency, we rely more on abstract locks (boosting) to expose concurrency within records.

For high skew, even without transactions we see a steady drop in performance simply due to serializing operations on the few shards unlucky enough to hold the especially popular keys.
However, combining is able to save the day here, because at high skew, there is a very good chance of finding operations to combine with, offloading significant load from the otherwise-overloaded popular shards. Our implementation of combining requires operations to be split into the acquire and commit phases, so it cannot be used without transactions. Though distributions during normal execution are generally more moderate, extreme skew models behavior during exceptional situations like BuzzFeed's viral dress.

Now that we have looked at various degrees of artificial contention, our next benchmarks exhibit contention modelled on real-world scenarios.

## RUBiS

~ Fig { #plot-bid-dist caption="Our modified Rubis models real-world auctions with bids per item (*left*) following a power law, and the frequency of bids over the auction time window (*right*), with higher bidding rates closer to closing time." }
![Rubis distributions](plots/bid-dist.pdf){.onecol}
~

~ Fig { #plot-rubis caption="Throughput of Rubis. Contention between bids on popular auctions, especially close to their closing time, causes performance to drop for r/w locks, but bids commute, so boosting is able to maintain high throughput even under heavy bidding." }
![Rubis](plots/rubis-tput.pdf){ .onecol }
~

~ Fig { #plot-rubis-conflicts caption="Breakdown of conflicts between Rubis transactions (minor contributors omitted) with 256 clients on bid-heavy workload (averaged). As predicted by [#fig-rubis-conflicts], boosting drastically reduces Bid-Bid conflicts, and phasing drastically reduces the remaining conflicts." }
![Rubis](plots/rubis-conflicts.pdf){ .onecol }
~

~ Fig { #plot-rubis-samples caption="Trace of throughput over time for Rubis (both with phasing). The workload is characterized by bursts of high-contention bidding activity. Boosting smooths out the performance, reducing variability by 2.8x."}
![Rubis sampled throughput](plots/rubis-samples.pdf){.onecol}
~

The RUBiS benchmark [@Amza:02] imitates an online auction service similar to those described back in [#sec-realworld]. There are 8 different kinds of transactions: `OpenAuction`, `ViewAuction`, `CloseAuction`, `Bid`, `Browse`, `AddComment`, `AddUser`, `ViewUser`, but `ViewAuction` and `Bid` dominate the workload. The benchmark specifies a workload consisting of a mix of these transactions and the average bids per auction that should result. However, the *distribution* of bids (by item and time) was unspecified.

Our modified implementation more accurately models the bid distributions observed by subsequent studies [@Akula:04;@Akula:07], with bids per item following a power law distribution and the frequency of bids increasing exponentially at the end of an auction ([#plot-bid-dist] shows these distributions for one of our runs).
Otherwise, we follow the parameters specified in [@Amza:02]: 30,000 items, divided into 62 regions and 40 categories, with an average of 10 bids per item, though in our case this is distributed according to a zipfian distribution with $\alpha = 1$.

[#plot-rubis] shows the performance results for two different workloads: read-heavy (10% Bid transactions), and bid-heavy (50% bid transactions). 
For the read-heavy workload, phasing is sufficient to provide reasonable performance, as bids do not often come in at a high enough rate to require commutativity. However, during times of high bidding activity, modeled by our bid-heavy workload, it becomes important to allow bids to commute with one another, we are able to maintain the same throughput as the read-heavy workload, roughly 2x better than r/w locks and 68% of the non-transactional throughput. Considering the importance of getting bids correct, this seems like an acceptable tradeoff.

Our goal with [#plot-rubis-conflicts] is to compare our predictions of conflicts from [#fig-rubis-conflicts] with reality. We look at a subset of the conflicts between Rubis transactions, plotted with a log scale because of the extreme change in conflicts that occurs with our optimizations.
The baseline is dominated by Bid-Bid conflicts which are drastically reduced by boosting (conflicts on `ItemBids` have gone away, the remaining conflicts come from other records not included in our simplified representation). On the other hand, the Bid-View conflicts have gone up; now that bids commute, there are more bids going through, so more chances of `ViewAuction` conflicting with them. Finally, we can see that with phasing and combining, all of the conflicts are further reduced.

Another interesting feature of auctions is the spikes in contention due to auctions closing, observed by prior work [@Akula:04]. By looking at a trace of throughput over the execution of our benchmark ([#plot-rubis-samples]), we observe those contention spikes as momentary drops in throughput. We see less variance over time with boosting enabled because the concurrency between bid transactions allows the datastore to keep up with the increased demand better.

Overall, we can see that boosting and phasing are crucial to achieving reasonable transaction performance in Rubis even during heavy bidding. If an auction service is unable to keep up with the rate of bidding, it will result in a loss of revenue and a lack of trust from users, so a system like Claret could prove invaluable to them.

## Retwis

~ Fig { #plot-retwis-dists caption="Power-law distributions in our Retwis benchmark. Distribution of followers per user (*left*) comes from the Kronecker synthetic graph generator. Number of reposts per post (*right*) is a result of the graph structure and our user model which favors reposting already-popular posts." }
![Retwis power-law distributions](plots/retwis-dists.pdf){ .onecol }
~

~ Fig { #plot-retwis caption="Throughput of Retwis. Boosting is essential during heavy posting because network effects lead to extreme contention on some records. Non-phasing results elided due to too many failed transactions." }
![Retwis](plots/retwis-tput.pdf){ .onecol }
~

Retwis is a simplified Twitter clone designed originally for Redis [@redis]. Data structures such as lists and sets are used track each user's followers and posts and keep a materialized up-to-date timeline for each user (represented as a sorted set). On top of Retwis's basic functionality, we added a "repost" action that behaves like Twitter's "retweet". Not being a true benchmark itself, Retwis doesn't specify a workload, so we simulate a realistic workload using a synthetic graph with power-law degree distribution and a simple model for user behavior for posting and reposting.

For our synthetic graph, we use the Kronecker graph generator from the Graph 500 benchmark [@graph500] which is designed to produce the same power-law degree distributions found in natural graphs. We use a Kronecker graph of approximately 65,000 users, with an average number of followers of 16 (scale 16 with an edge-factor of 16). [#plot-retwis-dists] (left) shows that the distribution of followers per user follows a power law as intended.

We use a simple model of user behavior to determine when and which posts to repost. Each time we load the most recent posts in a timeline for a random user (uniformly selected), they are sorted by the number of times they have already been reposted, and a discrete geometric distribution (skewed toward 0) is used to select the number of these to repost. The distribution of reposts resulting from our model is shown in [#plot-retwis-dists] (right), which follows a power law distribution: a decent approximation of the "viral" propagation effect observed in real social networks. Note that a small number of posts are reposted so much that they end up on over a quarter of users' timelines.

[#plot-retwis] shows throughput on two different workloads. Read-heavy models steady-state Twitter traffic, while the post-heavy workload models periods of above-average posting, such as during live events. 
We show only the results with phasing because the non-phasing baseline had too many transactions it was unable to complete. On the read-heavy workload, which models steady-state Twitter traffic, r/w locks are able to keep up reasonably well; after all, reading timelines is easy as long as they do not change frequently.
However, the post-heavy workload shows that when contention increases, the performance of r/w locks falls off much more drastically than with boosting. Combining even appears to pay off when there are enough clients to find matches.

Unlike auctions, Twitter is typically characterized as a non-transactional application because users are willing to tolerate minor inconsistencies. This tradeoff is clear when performance is as flat as the reader/writer performance is in these plots. However, when we look at the non-transactional throughput in these plots, we see that Claret achieves up to 82% of its throughput. Even without transactions, it suffers from the additional work caused by the higher ratio of posting, causing throughput (in terms of completed actions) to go down overall during high posting. With performance that close, it may be worth considering the benefits of using stronger consistency more often.

This embodies the philosophy behind Claret: if the true concurrency in the application can be exposed, then correctness need not be sacrificed, and applications which require correctness, like auctions, need not suffer for it. By leveraging the data structures programmers naturally choose to use, Claret exposes concurrency without burdening developers. It is worth noting that these experiments do not include weaker consistency that can be enabled by replicating shards in an eventually consistent way. Leveraging ADTs in such settings is something we plan on investigating next.
