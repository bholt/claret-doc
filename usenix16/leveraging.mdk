# Leveraging data types {#sec-leveraging}

<!--
~ Fig { #fig-adt-bid caption="Claret programming model example showing the ADT for a scored set and how it can be used to implement the Bid transaction from [#fig-levels]." }
![Example ADT version of Bid transaction](fig/adt-bid.pdf){.onecol}
~
-->

<!--
- adts hide their implementation, allow reasoning about abstract behavior
- if abstract view allows reordering, then concurrency control can take advantage of that
- use adts to express application-level properties
 -->

<!-- At the application level, actions like `Bid` may obviously commute with one another, but once these actions have been broken down into individual `put` and `get` operations, that knowledge is lost. The datastore no longer has any way to see that which reorderings are allowable, so adding multiple items to a `Set`, for instance, just looks like a series of writes with some implicit order. We need some way to communicate these higher-level properties about the application to the storage and execution system. -->

<!-- Abstract data types ... because they allow reasoning about the abstract behavior -->
<!-- are a natural and succinct way to implement applications.  -->

<!-- As was alluded to earlier, data types hold the key. Specifically, the notion of  -->

In Claret, programmers express application-level semantics by choosing the most specific ADT for their needs, either by choosing from the built-in ADTs ([#tab-adt-list]) or implementing their own (see [#sec-expressing]). In [#fig-levels], we saw that `Bid` transactions should commute; we just need know the current high bid.
A `topk` set meets our needs: it associates a score with each item, but is optimized to track those with the highest scores. Because `topk.add` operations commute, `Bid` transactions no longer conflict.

Abstract data types decouple their abstract behavior from their low-level concrete implementation.
Abstract operations can have properties such as commutativity, associativity, or monotonicity, which define how they can be reordered or executed concurrently, while the concrete implementation takes care of performing the necessary synchronization.

Knowledge of these properties can be used by the datastore in many ways to improve performance. First, we will show how commutativity can be used in the concurrency control system to avoid false conflicts (*boosting*) and ordering constraints (*phasing*). Then we will give an example of how associativity can be applied to reduce the load on the datastore (*combining*).

## Transaction boosting {#sec-boosting}

To ensure strong isolation, all transactional storage systems implement some form of concurrency control. A common approach is strict two-phase-locking (S2PL), where a transaction acquires locks on all records in the execution phase before performing any irreversible changes. 
Reader/writer locks are commonly used to allow transactions reading the same record to execute concurrently, but force transactions writing the record to wait for exclusive access.

*Abstract locks* [@Schwarz:84;@Weihl:1988;@Herlihy:88;@Badrinath:92;@Chrysanthis:91] generalize reader-writer locks to any operations that can logically run concurrently. When associated with an ADT, they allow operations that commute to hold the lock at the same time.
For the `topk` set, `add` operations can all hold the lock at the same time, but reading operations such as `size` must wait. The same idea can be applied to OCC: operations only cause conflicts if the abstract lock doesn't allow them to execute concurrently with other outstanding operations.

Using abstract locks to improve concurrency in transactions is known as *transaction boosting* [@Herlihy:PPoPP08] in the transactional memory community. However, this technique can be even more valuable to distributed transactions because their performance depends on how long locks are held. Waiting for one lock causes a cascade of waiting as others wait on the locks the blocked transaction holds. Every operation that can share an abstract lock reduces the time the rest of its locks are held for, which can have a big impact on performance.
In OCC-based systems, boosting would reduce the abort rate because fewer operations conflict with one another.

Boosting is essential for highly contended records, such as bids on a popular auction as it is about to close. If all the bids are serialized because they are thought to conflict, then fewer can complete and the final price could be lower. Using a `topk` set whose `add` operations naturally commute allows more bids to complete.

## Phasing {#sec-phasing}

Sometimes the order that operations happen to arrive causes problems with abstract locks. In particular, they only help if the operations that commute with each other arrive together; poor interleaving can result in little effect. *Phasing* reorders operations so that commuting operations execute together. This is a generalization of a recent technique called *phase reconciliation* [@Narula:OSDI14].

Each ADT defines a *phaser* that is responsible for grouping operations into *phases* that execute together. The interface will be described in more detail in [#sec-phaser]. Each record (or rather, the lock on the record), has its own phaser which keeps track of which mode is currently executing and keeps queues of operations in other modes waiting to acquire the lock. The phaser then cycles through these modes, switching to the next when all the operations in a phase have committed. Abstract locks, which have more distinct modes, benefit more than reader/writer locks, which must still serialize all writes.

By reordering operations, phasing has an effect on fairness. Queueing on locks improves fairness compared to baseline retry strategy that can lead to starvation. However, because phasing allows operations that commute to be executed before earlier blocked operations, it can lead to fairness issues. If a record has a steady stream of read operations, it may never release the lock to allow mutating operations. To prevent this, we cap phases at a maximum duration whenever there are blocked operations. In our experiments, we only observed this as an issue at extreme skew. The latency of some operations may increase as they are forced to wait for their phase to come. However, reducing conflicts often reduces the latency of transactions overall.

## Combining {#sec-combining}

*Associativity* is another useful property of operations. If we consider *commutativity*, used in boosting and phasing above, as allowing operations to be executed in a different order on a record, then *associativity* allows us to merge operations together *before* applying them to the record. This technique, *combining* [@flatCombining; @yew:combining-trees; @funnels], can drastically reduce contention on shared data structures and improve performance whenever the combined operation is cheaper than all the individual operations. In distributed settings, it is even more useful, effectively distributing synchronization for a single data structure over multiple hosts [@flat-combining-pgas13].

For distributed datastores, where the network is typically the bottleneck, combining can reduce server load. If many clients wish to perform operations on one record, each of them must send a message to acquire the lock. Even if they commute and so can hold the lock concurrently, the server handling the requests can get overloaded. In our model, however, "clients" are actually frontend servers handling many different end-user requests. With combining enabled, Claret keeps track of all the locks currently held by transactions on one frontend server. Whenever a client performs a combinable operation, if it finds its lock already in the table, it simply merges with the operation that acquired the lock, without needing to contact the server again.

For correctness, transactions sharing combined operations must all commit together. This also means that they must not conflict on any of their other locks, otherwise they would deadlock, and this applies transitively through all combined operations. Claret handles this by merging the lock sets of the two transactions and aborting a transaction and removing it from the set if it later performs an operation that conflicts with the others.

Tracking outstanding locks and merging lock sets adds overhead but offloads work to clients, which are easier to replicate to handle additional load than datastore shards. In our evaluation ([#sec-eval]), we find that combining is most effective at those critical times of extreme contention when load is highly skewed toward one shard.
