# Introduction
<!--
~~ Notes
- conditions for distributed applications running in datacenters are constantly changing
	- applications in datacenters, geo-replicated, exposed to the Internet, have to be able to handle a wide range of conditions
	- plan for worst-case conditions
	- provide structure/discipline to ensure programmers have handled all the things that can go wrong
	- provide system with hints about what it should prioritize, and release valves to allow it to still be useful when times are tough
	- robust to changes in the environment
- Applications have performance requirements
	- Sometimes explicit in the form of SLAs, promising a certain latency or availability
	- Sometimes more implicit (i.e. every additional ms of latency reduces revenue)
- Constantly balancing performance vs correctness / programmability
	- If it isn't scaling well, or latencies are too high, then relax consistency in some places and hope...
- This is error prone: every time you change consistency, there are new reorderings and conditions to consider
	- new edge cases to handle, *implicit* in the consistency model
	- accidentally leak into places that weren't intended to be weakened
- Worse: conditions can change at any moment; node goes down, network unreliable, traffic surges
	- In test environment, inconsistency is typically unlikely
	- Adverse conditions in production can cause errors that never appeared in testing, or are very difficult to test for
	- No way to know if you've caught them all
- Furthermore, when conditions are good, there's no need to resort to weak consistency
- It would be great if we had a way to:
	- Express performance bounds
		- Have the system help achieve them
	- Make inconsistency explicit and restricted
		- handle different cases in a disciplined way
		- restrict possible values, and where they can be used
- So the question is: *where to introduce this abstraction?*
	- As part of the data type!
	- Couples the effects of mutating operations with reads
	- Concise and modular: re-use data types, no annotations on individual operations
	- Safe: inconsistency expressed as return types
~~
-->

To provide good user experiences, modern datacenter applications and web services must balance many competing requirements. At a minimum, they must be scalable, highly available, and fault tolerant. 
On top of that, they often wish to guarantee certain response times to meet contractual service level agreements (SLAs) or to keep user engagement high; for example, Microsoft, Amazon and Google have all noted that every additional millisecond of latency translates directly to a loss in traffic and revenue [@linden06:amazon;@forrest09:slow]. On the other hand, every application has correctness criteria that can be hard requirements, such as never double-charging a user for a purchase, or a soft requirement, such as always showing the most recent tweets.

Developers of these applications typically balance these performance and correctness requirements by trading off consistency. To that end, the replicated NoSQL datastores used to implement these services, such as Cassandra [@cassandra] and Riak [@riak], often support multiple levels of consistency: from *linearizability* all the way down to *eventual consistency*. Whenever part of an application is not scaling well, or response latencies are unacceptably high, developers can choose to perform some operations with weaker consistency. However, they must then understand what can go wrong now that new reorderings of operations are possible and handle them. Failure to do so can lead to lost updates, duplications, stale data, and more. Relaxed consistency models are notoriously difficult to understand, especially when an application intertwines several models for different records. It is dangerously easy for inconsistencies to leak into operations that were intentionally kept as strongly consistent.

Even once an application's consistency model is tuned and tested to meet performance targets for one execution environment, the conditions in which web services operate is in constant flux. Sharing resources with many other co-located services, each of which also undergoes occasional garbage collection or operating system pauses, leads to unpredictable performance. Moreover, traffic coming in, from the outside world or via other services, can be highly unpredictable: when a black and blue dress goes viral [@buzzfeed-dress], or Justin Bieber posts a selfie [@bieber-instagram], the web service is subject to highly irregular load that overloads some nodes. In unusual situations (such as the World Cup or a natural disaster), traffic usually restricted to a single datacenter may cross multiple geo-replicated datacenters, with significantly different performance and consistency characteristics.

Using today's datastore interfaces, it is difficult and often impossible to handle this wide variety of conditions with a single application. Interfaces to datastores such as Cassandra offer consistency control at the level of individual operations, but provide no support for application-level reasoning about consistency [todo: smart people check this]. Failed by these abstractions, developers face a choice. One option is to settle for strong consistency, offering correct execution most of the time, but overheads so high --- 2--10$\times$ slower --- that the service falls over under even slightly adverse conditions. The other option is to pick a worst-case scenario and try to build their application to handle it, but at high risk of inconsistency causing application failures even during normal, good conditions, and with significant productivity costs.

We need a better programming model to help developers build fast and correct distributed applications that adapt to changing conditions. Current abstractions for distributed datastores do not offer the level of safety and abstraction that developers expect from modern programming languages. Unfortunately, the CAP theorem [@Brewer:CAP;@Gilbert:CAP] tells us that developers will always need to balance correctness against performance. But the right programming language support can help developers to make these tradeoffs efficiently and, most importantly, safely. Moreover, programming language support for application-level reasoning about consistency offers semantic information that opens new runtime optimizations that are unavailable to applications where consistency levels are specified at the per-operation level.

We propose the *inconsistent, performance-bound, approximate (IPA)* programming model for distributed datastores. The IPA model provides a type system for which type safety implies *consistency safety*: values from weakly consistent operations cannot flow into stronger consistency operations without explicit endorsement. Applications built with the IPA model allow developers to annotate *abstract data types* with performance targets, so the system can automatically adapt to meet them when conditions change. IPA annotations also express where incorrectness is acceptable, to give the system a release valve where it can relax consistency to meet those performance goals. The keys to our model are *IPA types* that express the consistency and correctness of any potentially inconsistent values. Type checking ensures that programmers handle potential error cases, and subtyping allows applications to make decisions based on the actual consistency of data.

We present the IPA model instantiated with types and annotations that implement two important distributed runtime mechanisms: latency-bound operations, and a novel *error tolerance* reservation system. We describe how these mechanisms can be implemented in a distributed environment based on Cassandra, and explain how the IPA programming model allows the system to trade off performance and consistency, safe in the knowledge that the type system has checked the program for consistency safety. We demonstrate experimentally that these mechanisms allow applications to dynamically adapt the correctness and performance trade-off with data-structure microbenchmarks and two applications: a simple Twitter clone based on Retwis [@retwis] and a Ticket sales service modelled after FusionTicket [@FusionTicket]. Our results show that IPA applications adapt to changing execution environments while respecting application-level correctness requirements.
