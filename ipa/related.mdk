# Related Work

## Consistency Models
A vast number of consistency models have been proposed over the years. From Lamport's *sequential consistency* [@Lamport:79:SC] and Herlihy's *linearizability* [@Herlihy:90:Linear] on the strong side, to *eventual consistency* [@Vogels:EC] at the other extreme. A variety of intermediate models fit elsewhere in the spectrum, each making different trade-offs balancing high performance and availability against ease of programming. Session guarantees, including *read-your-writes*, strengthen ordering for individual clients but reduce availability [@Terry:94:Session].
Many datastores support configuring consistency at a fine granularity: Cassandra [@cassandra] per operation, Riak [@riak] on an object or namespace granularity, as well as others [@Li:12:RedBlue;@Sovran:11:Walter].

The Conit consistency model [@Yu:02:Conit] explores the spectrum between weak and strong consistency, breaking consistency down along three axes: numerical error, order error, and staleness, with algorithms to bound each of them. The programming model, however, requires annotating each operation and making dependencies explicit in order to track these metrics, rather than annotating ADTs as in IPA.

## Explicit correctness requirements
Some programming models have gone beyond plain consistency models and allowed programmers to express correctness criteria directly.
Quelea [@Sivaramakrishnan:15:Quelea] has programmers write *contracts* to describe  *visibility* and *ordering* constraints between operations, then the system automatically selects the consistency level for each operation necessary to satisfy all the contracts. The contracts are independent of any particular consistency hierarchy; applications are portable, provided the constraint solver can find a solution given the consistency levels supported by the target platform. Quelea encourages programmers to use contracts to describe the semantics of application-specific datatypes which can later be reused.

In Indigo [@Balegas:15:Indigo], programmers write *invariants* over abstract state and state transitions and annotate post-conditions on actions to express their side-effects in terms of the abstract state. With these in place, the system can determine where in the program these invariants could be violated and adds coordination logic to prevent it. Indigo uses a similar reservation system to enforce numeric constraints. Neither Indigo nor Quelea, however, allow programmers to specify approximations or error tolerances, nor do they enforce any kind of performance bounds.


## Explicit performance bounds
The IPA model's latency-bound policies were inspired by Pileus's *consistency-based SLAs* [@Terry:13:SLAs]. Consistency SLAs specify a target latency and consistency level (e.g. 100 ms with read-my-writes), associated with a *utility*. Each operation specifies a set of SLAs, and the system predicts which is most likely to be met, attempting to maximize utility, and returns both the value and the achieved consistency level. 

Consistency SLAs are more expressive than IPA's latency bounds, allowing multiple acceptable targets to be indicated and weighted. On the other hand, the IPA type system provides more assistance in working with the results: `Rushed[T]` values can statically ensure that all possible cases are handled, and protect programmers from inadvertently using weak values where they should not. Additionally, Pileus's SLAs are specified on individual read operations, a greater annotation burden, and preventing the potential optimizations to writes that come from coupling the two at the ADT level.

<!--
Probabilistically bounded staleness (PBS) [@Bailis:12:PBS] are a form of performance and correctness bound. Using a predictive model, PBS is able to quantify the degree of inconsistency that is likely for a given operation, but it has not been used to directly improve programming models. This kind of knowledge could be applied within IPA's runtime system to make better latency predictions or to provide a new IPA probabilistic IPA type similar to `Interval[T]`.
-->

A long history of systems have been built around the principle that applications may be willing to tolerate slightly stale data in exchange for improved performance, including databases [@roehm02:_fas;@plattner04:_ganym;@bernstein06:_relax_curren_serial_for_middl;@pu91:_replic_contr_distr_system] and distributed caches [@ports10:_trans_consis_autom_manag_applic_data_cache;@olston99:_adapt_precis_settin_cached_approx_values]. These systems generally require developers to explicitly specify staleness bounds on each transaction in terms of absolute time (although Bernstein et al.'s model can generate these from error bounds when a value's maximum rate of change is known). As a result, they are largely orthogonal to our work: these techniques can be used to build a datastore for IPA types, whereas our work introduces a type system to help developers manage their consistency requirements.

<!-- [todo: are there other systems with explicit performance bounds enforced by the system?] -->


<!-- ## Controlling staleness -->
<!-- Most eventually consistent models provides no guarantees about how long it will take for updates to propagate. However, there are several techniques to help bound the staleness of reads. -->

<!-- *Leases* are an old technique that essentially gives reads an *expiration date*: the datastore promises not to modify the value that was just read until the lease term is over. First proposed to avoid explicit invalidations in distributed file system caches [@Gray:89], leases have since been used in a multitude of ways: in Facebook's Memcache system [@Nishtala:13:Memcache] for invalidations, Google's Chubby [@Burrows:2006:Chubby] and Spanner [@Spanner] to adjust the frequency of heartbeat messages, and on mobile clients with exo-leases [@Shrira:08:ExoLeasing]. Warranties [@Liu:14:Warranties] are a generalization of leases, allowing arbitrary assertions over state or behavior. [todo: explain how our leases relate (if they get implemented)] -->

## Types for distributed systems
*Convergent* (or *conflict-free*) *replicated data types* (CRDTs) [@Shapiro:SSS11:CRDT] are data types designed for eventual consistency. Similar to how IPA types express weakened semantics which allow for implementation on weak consistency, CRDTs guarantee that they will converge on eventual consistency by forcing all update operations to commute. For example, Set `add` and `remove` typically do not commute, but a CRDT called an OR-Set re-defines them so that `add` wins over `remove`, making them commute again. CRDTs can be enormously useful because they allow concurrent updates with sane semantics, but they are still only eventually (or causally) consistent, so users must still deal with temporary divergence and out-of-date reads, and they do not incorporate performance bounds or variable accuracy.

Bloom [@Alvaro:11:Bloom;@Conway:12:BloomL;@Alvaro:14:Blazes] is a language and runtime system for defining whole applications that are guaranteed to converge. Based around a conceptual monotonically growing set of facts, the language encourages coordination-free computation, but automatically creates synchronization points where necessary.

## Types for approximation
As discussed in [#sec-types], IPA's programming model bears similarities with the type systems developed for *approximate computing*, in which values are tagged with their accuracy. EnerJ [@Sampson:11:EnerJ] and Rely [@Carbin:13:Rely] track the flow of approximate values to prevent them from interfering with precise computation. Chisel [@Misailovic:14:Chisel] and DECAF [@Boston:15:DECAF] extend this information-flow tracking to automatically infer the necessary accuracy of a computation's inputs to achieve application-level quality guarantees. IPA's interval types are similar in motivation to Uncertain&lt;T&gt;'s probability distributions [@Bornholt:14:UncertainT] and to a long line of work on interval analysis [@Moore:66:IA]. IPA targets a different class of applications to these existing approaches. The key difference is the source of approximation: rather than lossy hardware that can lose the true value forever, inconsistent values can be strengthened if necessary by forcing additional synchronization.

Approximate type systems, including IPA, are inspired by a long line of work on information flow tracking (e.g., [@myers99:_jflow;@sabelfeld03:_languag_based_infor_flow_secur;@denning77:_certif]). These systems use a combination of static type checking and dynamic analysis to enforce isolation between sensitive data and untrusted channels, guaranteeing a *non-interference* policy [@goguen82:_secur_polic_secur_model]: public information will never be influenced by confidential data.
