# Evaluation

The goal of the IPA programming model and runtime system is to build applications that adapt to changing conditions, performing nearly as well as weak consistency but with stronger consistency and safety guarantees. To that end, we evaluate our prototype implementation under a variety of network conditions using both a real-world testbed (Google Compute Engine [@GoogleCompute]) and simulated network conditions. We start with microbenchmarks to understand the performance of each of the runtime mechanisms independently. We then study two applications in more depth, exploring qualitatively how the programming model helps avoid potential programming mistakes in each and then evaluating their performance against strong and weakly consistent implementations.

## Simulating adverse conditions
<!-- The purpose of IPA's dynamic policies is to allow programs to adapt to adverse conditions, so to evaluate them, we must subject them to a variety of network conditions and see how well they handle each. 
Evaluating replicated datastores under adverse conditions is challenging: tests conducted in a well-controlled environment where network latencies are low and variability is negligible will yield little of interest, whereas tests in production environments involve so many free variables that deciphering the results and reproducing them is difficult.
An alternative approach is to *simulate* a variety of environments chosen to stress the system or mimic real challenging situations. -->

To control for variability, we perform our experiments with a number of simulated conditions, and then validate our findings against experiments run on globally distributed machines in Google Compute Engine.
We use a local test cluster with nodes linked by standard ethernet and Linux's Network Emulation facility [@netem] (`tc netem`) to introduce packet delay and loss at the operating system level. We use Docker containers [@docker] to enable fine-grained control of the network conditions between processes on the same physical node.

[#tab-conditions] shows the set of conditions we use in our experiments to explore the behavior of the system. The *uniform 5ms* link simulates a well-provisioned datacenter; *slow replica* models contention or hardware problems that cause one replica to be slower than others, and *geo-distributed* replicates the latencies between virtual machines in the U.S., Europe, and Asia on Amazon EC2 [@AmazonEC2]. These simulated conditions are validated by experiments on Google Compute Engine with virtual machines in four datacenters: the client in *us-east*, and the storage replicas in *us-central*, *europe-west*, and *asia-east*.
We elide the results for *Local* (same rack in our testbed) except in [#fig-twitter-model] because the differences between policies are negligible, so strong consistency should be the default there.

<!--
The environment in which these experiments are normally carried out, on isolated systems with predictable, low latencies,

In our evaluation, we wish to determine how these performance and correctness bounds 

that must handle unpredictable traffic coming in from the world, and run in a multi-datacenter environment with 

The IPA type system provides structure that helps programmers ensure their application handles all the 

The goals of IPA's abstractions are twofold: first, to give structure to help programmers ensure that their application can handle the variety of 

IPA's abstractions aim to give programmers better ways of controlling how their applications behave in order to make them robust to changes in the environment. 

The most important factor in evaluating a system like IPA is how it handles different adverse conditions. 
We wish to know how an application written using these techniques 
-->

~ Tab { #tab-conditions caption="Network conditions for experiments: latency from client to each replicas, with standard deviation if high. [\vspace{-14pt}]{input:texraw}" }
| Network Condition     | Latencies (ms)               |||
|:----------------------|:---------|:--------|:----------|
| **Simulated**         | *Replica 1* |*Replica 2*| *Replica 3* |
| Uniform / High load   | 5        | 5       | 5         |
| Slow replica          | 10       | 10      | 100       |
| Geo-distributed (EC2) | 1 ± 0.3  | 80 ± 10 | 200 ± 50  |
|-----------------------|----------|---------|-----------|
| **Actual**            | *Replica 1* |*Replica 2*| *Replica 3* |
| Local (same rack)     | <1       | <1      | <1        |
| Google Compute Engine | 30 ± <1  |100 ± <1 | 160 ± <1 |
|-----------------------|----------|---------|-----------|
{font-size: small}
[\vspace{-6pt}]{input:texraw}
~

## Microbenchmark: Counter
We start by measuring the performance of a simple application that randomly increments and reads from a number of counters with different IPA policies. Random operations (`incr(1)` and `read`) are uniformly distributed over 100 counters from a single multithreaded client (allowing up to 4000 concurrent operations).

### Latency bounds {#eval-latency-bounds}

~ Fig { #plot-counter-lbound caption="*Counter: latency bounds, mean latency.* Beneath each bar is the % of strong reads. Strong consistency is never possible for the 10ms bound, but 50ms bound achieves mostly strong, only resorting to weak when network latency is high. [\vspace{-8pt}]{input:texraw}" }
![](plots/counter_lbound.pdf){ .onecol }
[\vspace{-16pt}]{input:texraw}
~

~ Fig { #fig-counter-err .wide caption="*Counter benchmark: error tolerance.* In (a), we see that wider error bounds reduce mean latency because fewer synchronizations are required, matching *weak* around 5-10%. In (b), we see actual error of *weak* compared with the actual interval for a 1% error bound with varying fraction of writes; average error is less than 1% but *maximum* error can be extremely high: up to 60%. [\vspace{-16pt}]{input:texraw}" }
~~ SubFigureRow
~~~ SubFigure {#fig-counter-error-perf; font-size: small; caption="Mean latency (increment *and* read)."}
![](plots/counter_err_perf.pdf)
~~~
~~~ SubFigure {#fig-counter-error; font-size: small; caption="Observed % error for weak and strong, compared with the actual interval widths returned for 1% error tolerance."}
![](plots/counter_err.pdf)
~~~
~~
[\vspace{-10pt}]{input:texraw}
~

~ Fig { #plot-counter-lbound-tail caption="*Counter: 95th percentile latency.* Latency bounds keep tail latency down, backing off to weak when necessary. [\vspace{-12pt}]{input:texraw}" }
![](plots/counter_lbound_tail.pdf){ .onecol }
~

Latency bounds provide predictable performance while maximizing consistency. We found that when latencies and load are low it is often possible to achieve strong consistency. [#plot-counter-lbound] shows the average operation latency with strong and weak consistency, as well with both 10ms and 50ms latency bounds.

As expected, there is a significant cost to strong consistency under all network conditions. IPA cannot achieve strong consistency under 10ms in any case, so the system must always default to weak consistency.  With a 50ms bound, IPA can achieve strong consistency in conditions when network latency is low (i.e., the single datacenter case). Cassandra assigns each client to read at a different replica for load balancing, so, with one slow replica, IPA will attempt to achieve strong consistency for all clients but not succeed. In our experiments, IPA was able to get strong consistency 83% of the time. Finally, with our geo-distributed environment, there are no 2 replicas within 50ms of our client, so strong consistency is never possible within our bounds; as a result, IPA adapts to only attempt weak in both cases. 

[#plot-counter-lbound-tail] shows the 95th percentile latencies for the same workload. The tail latency of the 10ms bound is comparable to weak consistency, whereas the 50ms bound overloads the slow server with double the requests, causing it to exceed the latency 5% of the time. There is a gap between latency-bound and weak consistency in the geo-distributed case because the `weak` condition uses weak reads *and* writes, while our rushed types, in order to have the option of getting strong reads without requiring a read of `ALL`, must do `QUORUM` writes.

Note that, without consistency types, it would be challenging for programmers to handle the varying consistency of returned values in changing network conditions. However, IPA's type system not only gives programmers the tools to reason about different consistency levels, it enforces consistency safety.

### Error bounds
This experiment measures the cost of enforcing error bounds using the reservation system described in [#sec-reservations], and its precision.
Reservations move synchronization off the critical path: by distributing write permissions among replicas, reads can get strong guarantees from a single replica. 
<!-- However, this balance must be carefully considered when evaluating the performance of reservations, more so than the other techniques.  -->
Note that reservations impact write performance, so we must consider both
in our experiments.

[#fig-counter-err]a shows latencies for error bounds of 1%, 5%, and 10%, plotting the average of read *and* increment operations. As expected, tighter error bounds increase latency because it forces more frequent synchronization between replicas. The 1\% error bound provides most of the benefit, except in the slow replica and geo-distributed environments where it forces synchronization frequently enough that the added latency slows down the system.  5-10% error bounds provide latency comparable to weak consistency. In the geo-distributed case, the coordination required for reservations makes even the 10\% error bound 4$\times$ slower than weak consistency, but this is still 28$\times$ faster than strong consistency.

While we have verified that error-bounded reads remain within our defined bounds, we also wish to know what error occurs in practice. We modified our benchmark to observe the actual error from weak consistency by incrementing counters a predetermined amount and reading the value; results are shown in [#fig-counter-err]b. We plot the percent error of weak and strong against the actual observed *interval width* for a 1% error bound, going from a read-heavy (1% increments) to write-heavy (all increments, except to check the value).

First, we find that the mean interval is less than the 1\% error bound because, for counters that are less popular, IPA is able to return a more precise interval. At low write rate, this interval becomes even smaller, down to .5\% in the geo-distributed experiment.  Next, we find that the mean error with weak consistency is also much less than 1\%; however the maximum error that we observed is up to 60\% of the actual value. This result motivates the need for error bounded consistency to ensure that applications do not see drastically incorrect values from weakly consistent operations.  Further, using the `Interval` type, IPA is able to give the application an estimate of the variance in the weak read, which is often more precise than the upper bound set by the error tolerance policy.

## Applications
Next, we explore the implementation of two applications in IPA and compare their performance against Cassandra using purely strong or weak consistency on our simulated network testbed and Google Compute Engine. 

### Ticket service
Our Ticket sales web service, introduced in [#sec-background], is modeled after FusionTicket [@FusionTicket], which has been used as a benchmark in recent distributed systems research [@Xie:14:Salt;@Xie:15:ACIDAlt]. We support the following actions:

- `browse`: List events by venue
- `viewEvent`: View the full description of an event including number of remaining tickets
- `purchase`: Purchase a ticket (or multiple)
- `addEvent`: Add an event at a venue.

~ Fig { #tickets-code caption="*Ticket service* code demonstrating consistency types. [\vspace{-8pt}]{input:texraw}" }
![](fig/tickets-code.pdf)
[\vspace{-18pt}]{input:texraw}
~

[#tickets-code] shows a snippet of code from the IPA implementation which can be compared with the non-IPA version from [#fig-tickets]. 
Tickets are modeled using the `UUIDPool` type, which generates unique identifiers to reserve tickets for purchase. The ADT ensures that, even with weak consistency, it never gives out more than the maximum number of tickets, so it is safe to `endorse` the result of the `take` operation as long as one is okay with the possibility of a false negative.
Rather than just using a weak read as in the original example, in IPA we can bound the inconsistency of the remaining ticket count using an error tolerance annotation on the tickets pool. Now to compute the price of the reserved ticket, we call `getTicketCount` and get an `Interval`, forcing us to decide how to handle the range of possible ticket counts. We decide to use the `max` value from the interval to be fair to users; the 5% error bound ensures that we don't sacrifice too much profit this way.

To evaluate the performance, we run a workload modelling a typical small-scale deployment: 50 venues and 200 events, with an average of 2000 tickets each (gaussian distribution centered at 2000, stddev 500); this ticket-to-event ratio ensures that some events run out tickets. Because real-world workloads exhibit power law distributions [@YCSB], we use a moderately skewed Zipf distribution with coefficient of 0.6 to select events.

~ Fig { #plot-tickets caption="*Ticket service: mean latency*, ***log scale.*** Strong consistency is far too expensive (>10$\times$ slower) except when load and latencies are low, but 5% error tolerance allows latency to be comparable to weak consistency. The 20ms latency-bound variant is either slower or defaults to weak, providing little benefit. Note: the ticket `Pool` is safe even when weakly consistent. [\vspace{-16pt}]{input:texraw}"; page-align: top }
![](plots/tickets.pdf){ .onecol }
[\vspace{-18pt}]{input:texraw}
~

~ Fig { #plot-tickets-rate caption="*Ticket service:* throughput on Google Compute Engine globally-distributed testbed. Note that this counts *actions* such as `tweet`, which can consist of multiple storage operations. Because error tolerance does mostly weak reads and writes, its performance tracks *weak*. Latency bounds reduce throughput due to issuing the same operation in parallel. [\vspace{-12pt}]{input:texraw}"; page-align: top }
![](plots/tickets_google_rate.pdf){ .onecol }
[\vspace{-18pt}]{input:texraw}
~

[#plot-tickets] shows the average latency of a workload consisting of 70% `viewEvent`, 19% `browse`, 10% `purchase`, and 1% `addEvent`. We plot with a log scale because strong consistency has over 5$\times$ higher latency. The `purchase` event, though only 10% of the workload, drives most of the latency increase because of the work required to prevent over-selling tickets. We explore two different IPA implementations: one with a 20ms latency bound on all ADTs aiming to ensure that both `viewEvent` and `browse` complete quickly, and one where the ticket pool size ("tickets remaining") has a 5% error bound. We see that both perform with nearly the same latency as weak consistency. With the low-latency condition (*uniform* and *high load*), 20ms bound does 92% strong reads, 4% for *slow replica*, and all weak on both *geo-distributed* conditions.

[#plot-tickets] also shows results on Google Compute Engine *(GCE)*. We see that the results of real geo-replication validate the findings of our simulated geo-distribution results.

On this workload, we observe that the 5% error bound performs well even under adverse conditions, which differs from our findings in the microbenchmark.
This is because ticket `UUIDPool`s begin *full*, with many tokens available, requiring less synchronization until they are close to running out. Contrast this with the microbenchmark, where counters started at small numbers (average of 500), where a 5% error tolerance means fewer tokens.

### Twitter clone

~ Fig { #fig-twitter-model caption="Twitter data model with policy annotations, `Rushed[T]` helps catch referential integrity violations and `Interval[T]` represents  approximate retweet counts. [\vspace{-10pt}]{input:texraw}" }
![](fig/twitter.pdf)
[\vspace{-18pt}]{input:texraw}
~

~ Fig { #plot-twitter caption="*Twitter clone: mean latency (all actions).* The IPA version performance comparably with weak consistency in all but one case, while strong consistency is 2-10$\times$ slower. [\vspace{-10pt}]{input:texraw}" }
![](plots/twitter.pdf)
[\vspace{-16pt}]{input:texraw}
~

Our second application is a Twitter-like service based on the Redis data modeling example, Retwis [@retwis]. The data model is simple: each user has a `Set` of followers, and a `List` of tweets in their timeline. When a user tweets, the tweet ID is eagerly inserted into all of their followers' timelines. Retweets are tracked with a `Set` of users who have retweeted each tweet.

[#fig-twitter-model] shows the data model with policy annotations: latency bounds on followers and timelines and an error bound on the retweets. This ensures that when tweets are displayed, the retweet count is not grossly inaccurate. As shown in `displayTweet`, highly popular tweets with many retweets can tolerate approximate counts – they actually abbreviate the retweet count (e.g. "2.4M") – but average tweets, with less than 20 retweets, will get an exact count. This is important because for regular people, they will notice if a friend's retweet is not reflected in the count, whereas Ellen Degeneres's record-breaking celebrity selfie, which nearly brought down Twitter in 2014 [@ellenselfie], can scale because a 5% error tolerance on 3.4 million retweets provides significant slack.

The code for `viewTimeline` in [#fig-twitter-model] demonstrates how latency-bound `Rushed[T]` types can be destructured with a match statement. In this case, the timeline (list of tweet IDs) is retrieved with a latency bound. Tweet content is added to the store before tweet IDs are pushed onto timelines, so with strong consistency we know that the list of IDs will all be able to load valid tweets. However, if the latency-bound type returns with weak consistency (`Inconsistent` case), then this *referential integrity* property may not hold. In that case, we must guard the call to `displayTweet` and retry if any of the operations fails (e.g., if the retweet set wasn't created yet).

We simulate a realistic workload by generating a synthetic power-law graph, using a Zipf distribution to determine the number of followers per user. Our workload is a random mix with 50% `timeline` reads, 14% `tweet`, 30% `retweet`, 5% `follow`, and 1% `newUser`.

We see in [#plot-twitter] that for all but the local (same rack) case, strong consistency is over 3$\times$ slower. Our implementation, combining latency and error-bounds, performs comparably with weak consistency but with stronger guarantees for the programmer. Our simulated geo-distributed condition turns out to be the worst scenario for IPA's Twitter, with latency over 2$\times$ slower than weak consistency. This is because weak consistency performed noticeably better on our simulated network, which had one very close (1ms latency) replica that it used almost exclusively.
