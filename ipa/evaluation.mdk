# Evaluation

The goal of the IPA programming model and runtime system is to build applications that adapt to changing conditions, performing nearly as well as weak consistency but with stronger consistency and safety guarantees. To that end, we evaluate our prototype implementation under a variety of network conditions using both a real-world testbed (Google Compute Engine [@GoogleCompute]) and simulated network conditions. We start with simple microbenchmarks to understand the performance of each of the runtime mechanisms independently. We then study two applications in more depth, exploring qualitatively how the programming model helps avoid potential programming mistakes in each and then evaluating their performance against strong and weakly consistent implementations.

## Simulating adverse conditions
<!-- The purpose of IPA's dynamic policies is to allow programs to adapt to adverse conditions, so to evaluate them, we must subject them to a variety of network conditions and see how well they handle each. -->
Evaluating replicated datastores under adverse conditions is challenging: tests conducted in a well-controlled environment where network latencies are low and variability is negligible will yield little of interest, whereas tests in production environments involve so many free variables that deciphering the results and reproducing them is difficult.
An alternative approach is to *simulate* a variety of environments chosen to stress the system or mimic real challenging situations. We perform our experiments with a number of simulated conditions, and then validate our findings against experiments run on globally distributed machines in Google Compute Engine.

On our own test cluster with nodes linked by standard ethernet, we use Linux's Network Emulation facility [@netem] (`tc netem`) to introduce packet delay and loss at the operation system level. We use Docker containers [@docker] to enable fine-grained control of the network conditions between processes on the same physical node.

[#tab-conditions] shows the set of conditions we use in our experiments to explore the behavior of the system. We have a *uniform 5ms* condition to simulate latencies within a well-provisioned datacenter, *slow replica* which models imbalanced load or hardware problems that can cause one replica to be significantly slower to respond, and a condition mimicking geo-replication, with latency distributions based on measurements of latencies between virtual machines in the U.S., Europe, and Asia on Amazon EC2 [@AmazonEC2]. These simulated conditions are validated by experiments with real networks in Google Compute Engine, running virtual machines in four datacenters: the client in *us-east*, and the datastore replicas in *us-central*, *europe-west*, and *asia-east*.
We elide the results for *Local* (same rack in our own testbed) except in [#fig-twitter-model] because the differences between policies are negligible. In such situations, strong consistency ought to be the default.

<!--
The environment in which these experiments are normally carried out, on isolated systems with predictable, low latencies,

In our evaluation, we wish to determine how these performance and correctness bounds 

that must handle unpredictable traffic coming in from the world, and run in a multi-datacenter environment with 

The IPA type system provides structure that helps programmers ensure their application handles all the 

The goals of IPA's abstractions are twofold: first, to give structure to help programmers ensure that their application can handle the variety of 

IPA's abstractions aim to give programmers better ways of controlling how their applications behave in order to make them robust to changes in the environment. 

The most important factor in evaluating a system like IPA is how it handles different adverse conditions. 
We wish to know how an application written using these techniques 
-->

~ Tab { #tab-conditions caption="Network conditions for experiments: latency from client to each replicas, with standard deviation if high. [\vspace{-14pt}]{input:texraw}" }
| Network Condition     | Latencies (ms)               |||
|:----------------------|:---------|:--------|:----------|
| **Simulated**         | *Replica 1* |*Replica 2*| *Replica 3* |
| Uniform / High load   | 5        | 5       | 5         |
| Slow replica          | 10       | 10      | 100       |
| Geo-distributed (EC2) | 1 ± 0.3  | 80 ± 10 | 200 ± 50  |
|-----------------------|----------|---------|-----------|
| **Actual**            | *Replica 1* |*Replica 2*| *Replica 3* |
| Local (same rack)     | <1       | <1      | <1        |
| Google Compute Engine | 30 ± <1  |100 ± <1 | 160 ± <1 |
|-----------------------|----------|---------|-----------|
{font-size: small}
[\vspace{-6pt}]{input:texraw}
~

## Microbenchmark: Counter
We start by measuring the performance of a very simple application that randomly increments and reads from a number of counters with different IPA policies. Random operations (`incr(1)` and `read`) are uniformly distributed over 100 counters from a single multithreaded client (allowing up to 4000 concurrent operations).

### Latency bounds {#eval-latency-bounds}

~ Fig { #plot-counter-lbound caption="*Counter: latency bounds, mean latency.* Beneath each bar is the % of strong reads. Strong consistency is never possible for the 10ms bound, but 50ms bound achieves mostly strong, only resorting to weak when latency is high. [\vspace{-8pt}]{input:texraw}" }
![](plots/counter_lbound.pdf){ .onecol }
[\vspace{-16pt}]{input:texraw}
~

~ Fig { #plot-counter-lbound-tail caption="*Counter: 95th percentile latency.* Latency bounds keep tail latency down, backing off to weak when necessary. [\vspace{-12pt}]{input:texraw}" }
![](plots/counter_lbound_tail.pdf){ .onecol }
~

~ Fig { #fig-counter-err .wide caption="*Counter benchmark: error tolerance.* In (a), we see that wider error bounds reduce mean latency because fewer synchronizations are required, matching *weak* around 5-10%. In (b), we see actual error of *weak* compared with the actual interval for a 1% error bound with varying fraction of writes; average error is less than 1% but *maximum* error can be extremely high: up to 60%. [\vspace{-.2in}]{input:texraw}" }
~~ SubFigureRow
~~~ SubFigure {#fig-counter-error-perf; font-size: small; caption="Mean latency (increment *and* read)."}
![](plots/counter_err_perf.pdf)
~~~
~~~ SubFigure {#fig-counter-error; font-size: small; caption="Observed % error for weak and strong, compared with the actual interval widths returned for 1% error tolerance."}
![](plots/counter_err.pdf)
~~~
~~
[\vspace{-10pt}]{input:texraw}
~


Latency bounds aim to provide predictable performance for clients while attempting to maximize consistency. Under favorable conditions — when latencies and load are low — it is often possible to achieve strong consistency. [#plot-counter-lbound] shows the average latency of a counter with strong, weak, and two latency bounds under various conditions. We can see that there is a significant difference in latency between strong and weak. In these conditions, it is almost never possible to get strong consistency within 10ms, so the 10ms bound's monitor predicts it will not get strong consistency and falls back to weak consistency. 
For a 50ms bound, the counter is able to get strong consistency if network latency is low. However, with one slow replica (out of 3), there is a chance that the `QUORUM` read needed for strong consistency will hit the slow replica, so IPA attempts both; in this case, 82% got strong consistency, and 18% timed out and went with weak. Finally, with our simulated geo-distributed environment, there are no 2 replicas within 50ms of our client, so strong consistency is never possible within our bounds; as a result, IPA adapts to only attempt weak in both cases.

[#plot-counter-lbound-tail] shows the 95th percentile latencies of the same workload. We see that the tail latency of the 10ms bound is comparable to weak, though the 50ms bound guesses incorrectly occasionally for the case of the slow replica. We see a gap between latency-bound and weak in the geo-distributed case. This is because the `weak` condition uses weak reads *and* writes, while our rushed types, in order to have the option of getting strong reads without requiring a read of `ALL`, must do `QUORUM` writes.

### Error bounds
This experiment determines the cost of enforcing error bounds using the reservation system described in [#sec-reservations], and to determine how tight error bounds can be while providing performance comparable to weak consistency.
Reservations move synchronization off the critical path: by distributing write permissions among replicas, reads can get strong guarantees from a single replica. 
<!-- However, this balance must be carefully considered when evaluating the performance of reservations, more so than the other techniques.  -->
When evaluating the latency bounds, we considered only read latency because we didn't change the writes. Because reservations actually slow down writes, now we must consider both.

[#fig-counter-err]a shows latencies for error bounds of 1%, 5%, and 10%, plotting the average of reads *and* increment operations because reads along would always be equivalent to weak. We see that tighter bounds increase latency because it forces more synchronization, which must use consistency of `ALL`. In most conditions, 5-10% error bounds have latency comparable to weak, except geo-distributed, where it seems that our implementation of reservations is not a good solution.

While we have verified that error-bounded reads remain within our defined bounds, we also wish to know what error occurs in practice without the bounds. We modified our benchmark to be able to observe the error from weak consistency by incrementing counters a predetermined amount and observing the value; results are shown in [#fig-counter-err]b. We plot the percent error of weak and strong against the actual observed interval width for a 1% error bound, going from a read-heavy (1% increments) to write-heavy (all increments, except to check the value).

First, we find that the *mean* error is less than 1% – inconsistency is quite rare, though it may be higher in practice, when operations come from more than one client.
<!-- , but then we would not have been able to observe the error in the way we did. -->
However, even with this experiment, we see outliers with significant error when writing is heavy: up to 60% error in the geo-replicated case. Finally, it is worth noting that for read-heavy workloads, the interval width (green line) is less than 1% because it dynamically adjusted as reservations were not needed.

## Applications
Next, we explore the implementation of two applications in IPA and compare their performance with Cassandra using strictly strong or weak consistency on our simulated network testbed and Google Compute Engine. 

### Ticket service
Our Ticket sales web service, introduced in [#sec-background], is modeled after FusionTicket [@FusionTicket], which has been used as a benchmark in recent distributed systems research [@Xie:14:Salt;@Xie:15:ACIDAlt]. We support the following actions:

- `browse`: List events by venue
- `viewEvent`: View the full description of an event including number of remaining tickets
- `purchase`: Purchase a ticket (or multiple)
- `addEvent`: Add an event at a venue.

~ Fig { #tickets-code caption="*Ticket service* code demonstrating use of IPA types. [\vspace{-10pt}]{input:texraw}" }
![](fig/tickets-code.pdf)
[\vspace{-18pt}]{input:texraw}
~

[#tickets-code] shows a snippet of code from the IPA implementation which can be compared with the non-IPA version from [#fig-tickets]. 
Tickets are modeled using the `UUIDPool` type, which generates unique identifiers to reserve tickets for purchase. The ADT ensures that, even with weak consistency, it never gives out more than the maximum number of tickets, so it is safe to `endorse` the result of the `take` operation as long as one is okay with the possibility of a false negative. To compute the price of the reserved ticket, we now get back an `Interval` representing the range of possible remaining ticket counts, forcing our program to decide how to handle this range. We decide to use the `max` value from the interval to be conservative and fair to users; the 5% error bound ensures that we don't sacrifice too much profit this way.

To evaluate the performance, we run a workload modelling a typical small-scale deployment: 50 venues and 200 events, with an average of 2000 tickets each (gaussian distribution centered at 2000, stddev 500), with a ticket to event ratio that leads to some events running out tickets. Because real-world workloads exhibit power law distributions [@YCSB], we use a moderately skewed Zipf distribution with coefficient of 0.6 to choose events.

~ Fig { #plot-tickets caption="*Ticket service: mean latency*, ***log scale.*** Strong consistency is far too expensive (>10$\times$ slower) except when load and latencies are low, but 5% error tolerance allows latency to be comparable to weak consistency. The 20ms latency-bound variant is either slower or defaults to weak, providing little benefit. Note: the ticket `Pool` is safe even when weakly consistent."; page-align: top }
![](plots/tickets.pdf){ .onecol }
[\vspace{-18pt}]{input:texraw}
~

~ Fig { #plot-tickets-rate caption="*Ticket service:* throughput on Google Compute Engine globally-distributed testbed. Note that this counts *actions* such as `tweet`, which can consist of multiple storage operations. Because error tolerance does mostly weak reads and writes, its performance tracks *weak*. Latency bounds reduce throughput due to issuing the same operation in parallel. [\vspace{-12pt}]{input:texraw}"; page-align: top }
![](plots/tickets_google_rate.pdf){ .onecol }
[\vspace{-18pt}]{input:texraw}
~

[#plot-tickets] shows the average latency of a workload consisting of 70% `viewEvent`, 19% `browse`, 10% `purchase`, and 1% `addEvent`. We plot with a log scale because strong consistency has over 5$\times$ higher latency. The `purchase` event, though only 10% of the workload, drives most of the latency increase because of the work required to prevent over-selling tickets. We explore two different IPA implementations: one with a 20ms latency bound on all ADTs aiming to ensure that both `viewEvent` and `browse` complete quickly, and one where the ticket pool size ("tickets remaining") has a 5% error bound. We see that both perform with nearly the same latency as weak consistency. With the low-latency condition (*uniform* and *high load*), 20ms bound does 92% strong reads, 4% for *slow replica*, and all weak on both *geo-distributed* conditions.

[#plot-tickets] also shows results on Google Compute Engine *(GCE)*. We see that the results of real geo-replication validate the findings of our simulated geo-distribution results.

On this workload, we observe that the 5% error bound performs well even under adverse conditions, which differs from our findings in the microbenchmark.
This is because ticket `UUIDPool`s begin *full*, with many tokens available, requiring less synchronization until they are close to running out. Contrast this with the microbenchmark, where counters started at small numbers (average of 500), where a 5% error tolerance means fewer tokens.

### Twitter clone

~ Fig { #fig-twitter-model caption="Twitter application's (simplified) data model, with latency bounds for followers and timelines, and error tolerance for number of retweets (`size` of the retweets set). [\vspace{-10pt}]{input:texraw}" }
![](fig/twitter.pdf)
[\vspace{-18pt}]{input:texraw}
~

~ Fig { #plot-twitter caption="*Twitter clone: mean latency (all actions).* The IPA version is on-par with weak consistency in all but one of the conditions, while strong consistency is 2-10$\times$ slower. [\vspace{-10pt}]{input:texraw}" }
![](plots/twitter.pdf)
[\vspace{-16pt}]{input:texraw}
~

Our second application is a Twitter-like service based on the Redis data modeling example, Retwis [@retwis]. The data model is simple: each user has a `Set` of followers, a `Set` of users they follow, and a `List` of their tweets. Each user's timeline is kept materialized as a `List` of tweet IDs — when a user tweets, the new tweet ID is eagerly appended to all of their followers' timelines. Retweets are tracked with a `Set` of users who have retweeted each tweet. The retweet count, loaded for each tweet when a timeline is loaded, is represented in this ADT model by the size of the set.

Retweets are another good example of a place where error tolerance bounds faithfully represent an application-level correctness constraint. Most tweets by an average user on Twitter will have few retweets; in these situations, even a small error can be glaringly obvious: a user may get a notification, then look at the tweet and get frustrated when they cannot see the retweet. However, for a highly popular tweet that has been retweeted millions of times already, one more tweet does not need to be reflected immediately in the count. In fact, most views of Twitter will truncate large values to something like "3.4M", which is a perfect way to use an `Interval` result. Moreover, situations with massive numbers of retweets can be a performance bottleneck if not handled correctly, as Twitter learned when Ellen Degeneres's celebrity selfie brought it to a standstill at the 2014 Oscars [@ellenselfie].
For the user's timeline, on the other hand, we do not need to know the size, but we do want it to load quickly to keep users engaged, so we use a latency bound. Finally, the follower list, which is changed infrequently, could be left as strongly consistent to ensure that new followers get all the latest tweets, but we want to keep performance as a priority, so use another latency bound, which gives us the option to warn the user if they may need to refresh to get more tweets.

Retwis doesn't specify a workload, so we simulate a realistic workload by generating a synthetic power-law graph, using a Zipf distribution to determine the number of followers per user. Our workload is a random mix with 50% `timeline` reads, 14% `tweet`, 30% `retweet`, 5% `follow`, and 1% `newUser`.

We can see in [#plot-twitter] that for all but the local (same rack) case, strong consistency is over 3$\times$ slower, but our implementation combining latency and error-bounds performs comparably with weak consistency, but with stronger guarantees for the programmer. Our simulated geo-distributed condition turns out to be the worst-case scenario for IPA's Twitter, with latency over 2$\times$ slower than weak consistency. This is because weak consistency performed noticeably better on our simulated network, which had one very close (1ms latency) replica that it could use almost exclusively.
